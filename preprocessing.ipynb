{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, email\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import tokenize\n",
    "import pprint, re\n",
    "\n",
    "from fileRetrieval import path_list\n",
    "from fileToDataFrame import fileToDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディレクトリ 内のメールファイルを読み込む\n",
    "\n",
    "directory_path = \"enron/maildir/allen-p/\"\n",
    "\n",
    "file_list, dir_list=path_list(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストファイルをデータフレームに格納する\n",
    "\n",
    "df = fileToDataFrame(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phillip--To the extent that we can give Chair Hoecker our spin on the reasons \n",
      "for the hikes, we would like to.  The Commission is getting calls from \n",
      "legislators, DOE, etc. about the prices and is going to have to provide some \n",
      "response.  Better if it coincides with Enron's view and is not anti-market.  \n",
      "We still haven't decided what we will provide.  You definitely will be \n",
      "included in that discussion once we get the numbers from accounting.  Thanks.\n",
      "\n",
      "\n",
      "   \n",
      "\t\n",
      "\t\n",
      "\tFrom:  Phillip K Allen                           12/12/2000 12:03 PM\n",
      "\t\n",
      "\n",
      "To: Christi L Nicolay/HOU/ECT@ECT\n",
      "cc:  \n",
      "\n",
      "Subject: Talking points about California Gas market\n",
      "\n",
      "Christy,\n",
      "\n",
      " I read these points and they definitely need some touch up.  I don't \n",
      "understand why we need to give our commentary on  why prices are so high in \n",
      "California.  This subject has already gotten so much press.  \n",
      "\n",
      "Phillip\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------- Forwarded by Phillip K Allen/HOU/ECT on 12/12/2000 \n",
      "12:01 PM ---------------------------\n",
      "From: Leslie Lawner@ENRON on 12/12/2000 11:56 AM CST\n",
      "To: Christi L Nicolay/HOU/ECT@ECT, Joe Hartsoe/Corp/Enron@ENRON, Rebecca W \n",
      "Cantrell/HOU/ECT@ECT, Ruth Concannon/HOU/ECT@ECT, Stephanie \n",
      "Miller/Corp/Enron@ENRON, Phillip K Allen/HOU/ECT@ECT, Jane M \n",
      "Tholt/HOU/ECT@ECT, Richard Shapiro/NA/Enron@Enron\n",
      "cc:  \n",
      "Subject: Talking points about California Gas market\n",
      "\n",
      "Here is my stab at the talking points  to be sent in to FERC along with the \n",
      "gas pricing info they requested for the California markets.  Let me or \n",
      "Christi know if you have any disagreements, additions, whatever.  I am \n",
      "supposed to be out of here at 2:15 today, so if you have stuff to add after \n",
      "that, get it to Christi.  Thanks.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df['Message'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "def toSentences(message_text):\n",
    "    sentences = tokenize.sent_tokenize(message_text)\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        cleaned_sentence = ' '.join(sentence.split())\n",
    "        if cleaned_sentence[0].isupper()==True:\n",
    "            cleaned.append(cleaned_sentence)\n",
    "        else:\n",
    "            cleaned[-1] = cleaned[-1] + ' ' + cleaned_sentence\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentences = toSentences(df['Message'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.structured_prediction\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/openie-model.2020.03.26.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = {} #{sentence:predict_result}\n",
    "for i,s in enumerate(cleaned_sentences):\n",
    "    st[i] = predictor.predict(sentence = s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argIndexList(predict_result:dict):\n",
    "    triples = {}\n",
    "    for i,result in predict_result.items():\n",
    "        triples.setdefault(i,{})\n",
    "        for verb in result['verbs']:\n",
    "            ARG0 = [j for j,tag in enumerate(verb['tags']) if re.match('([BI]-)ARG0',tag)]\n",
    "            ARG1 = [k for k,tag in enumerate(verb['tags']) if re.match('([BI]-)ARG1',tag)]\n",
    "            if ARG0 != [] and ARG1 != []:\n",
    "                ARG0 = ' '.join([result['words'][l] for l in ARG0])\n",
    "                ARG1 = ' '.join([result['words'][m] for m in ARG1])\n",
    "                triples[i].setdefault(verb['verb'],(ARG0, ARG1))\n",
    "            else:\n",
    "                pass\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'like': ('we', 'to')},\n",
       " 1: {'getting': ('The Commission', 'calls'),\n",
       "  'provide': ('The Commission', 'some response')},\n",
       " 2: {},\n",
       " 3: {'decided': ('We', 'what we will provide'), 'provide': ('we', 'what')},\n",
       " 4: {'get': ('we', 'the numbers')},\n",
       " 5: {},\n",
       " 6: {'read': ('I', 'these points'), 'need': ('they', 'some touch up')},\n",
       " 7: {'understand': ('I',\n",
       "   'why we need to give our commentary on why prices are so high in California'),\n",
       "  'need': ('we',\n",
       "   'to give our commentary on why prices are so high in California'),\n",
       "  'give': ('we', 'our commentary on why prices are so high in California')},\n",
       " 8: {},\n",
       " 9: {'requested': ('they', 'the gas pricing info')},\n",
       " 10: {'know': ('me or Christi',\n",
       "   'if you have any disagreements , additions , whatever')},\n",
       " 11: {},\n",
       " 12: {}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argIndexList(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
