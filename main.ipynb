{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, email\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import tokenize\n",
    "import pprint, re, time\n",
    "\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loadFile import getFileList, getDirList, fileToDataFrame\n",
    "\n",
    "# ディレクトリ 内のメールファイルを読み込む\n",
    "directory_path = \"wiki-research-l/2020-July\"\n",
    "file_list = getFileList(directory_path)\n",
    "file_list.sort()\n",
    "\n",
    "# テキストファイルをデータフレームに格納する\n",
    "mail_df, thread_df = fileToDataFrame(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メールのBody部分を各パートに分解する\n",
    "bodies = {dict(Bodies=[])}\n",
    "for idx, body in mail_df.loc[:,'Body'].items():\n",
    "    origin = []\n",
    "    greetings = dict(Greetings=[])\n",
    "    sentences = dict(Sentence=[])\n",
    "    captions = dict(Caption=[])\n",
    "    bulletlist = dict(Bulletlist=[])\n",
    "    ending = dict(Ending=[])\n",
    "    quotation = dict(Quotation=[])\n",
    "    footer = dict(Footer=[])\n",
    "    misc = dict(Misc=[])\n",
    "\n",
    "    lines = body.splitlines()\n",
    "    for num, line in enumerate(lines):\n",
    "        if re.match(r'\\[G\\]',line) is not None:\n",
    "            greetings['Greetings'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[S\\]',line) is not None:\n",
    "            sentences['Sentence'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[C\\]',line) is not None:\n",
    "            captions['Caption'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[B\\]',line) is not None:\n",
    "            bulletlist['Bulletlist'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[E\\]',line) is not None:\n",
    "            ending['Ending'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[Q\\]',line) is not None:\n",
    "            quotation['Quotation'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[F\\]',line) is not None:\n",
    "            footer['Footer'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[M\\]',line) is not None:\n",
    "            misc['Misc'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        else: #空白行に対応する\n",
    "            continue\n",
    "    originbody = ' '.join(origin)\n",
    "    bodies['Bodies'].append({'idx':idx, 'countrows':len(lines), 'body':originbody, **greetings, **sentences, **captions, **bulletlist, **ending, **quotation, **footer, **misc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "for i in bodies['Bodies']:\n",
    "    if i['Sentence'] == []:\n",
    "        pprint.pprint(i)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "test = bodies['Bodies'][0]['body']\n",
    "doc = nlp(test)\n",
    "pprint.pprint([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#　bodiesのsentenceのvalueから複数文のテキストを作る\n",
    "s_list = []\n",
    "for i in bodies['Bodies']:  #1通ずつ取り出す\n",
    "    text = ''\n",
    "    for j in i['Sentence']: #1行{行番号:文}ずつ取り出し，複数文が含まれた1つの文字列にする\n",
    "        text = text + list(j.values())[0] + ' '\n",
    "    s_list.append(text) #1通ごとの自然文のテキストをリストに格納する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# センテンスのdataframeを作る\n",
    "sentence_list = []\n",
    "# 文章全体に対する前処理\n",
    "for i, content in enumerate(s_list):\n",
    "    sentences = tokenize.sent_tokenize(content)\n",
    "    # 文に対する前処理\n",
    "    for j, sentence in enumerate(sentences):\n",
    "        sentence = re.sub(r'\\s{2,}',' ',sentence)\n",
    "        sentence_list.append([mail_df['Message-ID'][i],sentence])\n",
    "sentence_df = pd.DataFrame(sentence_list,columns=['Message-ID','sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_df.to_csv('wiki-research-l/output/sentence_df.csv')\n",
    "len(set(sentence_df[\"Message-ID\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OpenIEにかける\n",
    "\n",
    "from pyopenie import OpenIE5\n",
    "extractor = OpenIE5('http://localhost:8000')\n",
    "\n",
    "extractions_list = []\n",
    "for i,sentence in sentence_df['sentence'].items():\n",
    "    try:\n",
    "        extractions = extractor.extract(sentence)\n",
    "        extractions_list.append(extractions)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OIEの抽出結果を整形する\n",
    "\n",
    "rows = []\n",
    "for i, extractions in enumerate(extractions_list):\n",
    "    if extractions == []:\n",
    "        pass\n",
    "    else:\n",
    "        for extraction in extractions:\n",
    "            message_id = sentence_df['Message-ID'][i]\n",
    "            sentence = extraction['sentence']\n",
    "            confidence = extraction['confidence']\n",
    "            arg1 = extraction['extraction']['arg1']['text']\n",
    "            rel = extraction['extraction']['rel']['text']\n",
    "            arg2s_list = []\n",
    "            for arg2 in extraction['extraction']['arg2s']:\n",
    "                arg2s_list.append(arg2['text'])\n",
    "            arg2s = ' '.join(map(str, arg2s_list))\n",
    "            row = [message_id, sentence, arg1, rel, arg2s, confidence]\n",
    "            rows.append(row)\n",
    "\n",
    "# 整形結果をdataframeにする\n",
    "kb_df = pd.DataFrame(rows, columns = ['message_id','sentence', 'arg1', 'rel', 'arg2s', 'confidence'])\n",
    "kb_df.head()\n",
    "# dataframeをcsv出力する\n",
    "kb_df.to_csv('wiki-research-l/output/triple_from_text_part.csv')\n",
    "\n",
    "# dataframeをRDBのテーブルにする\n",
    "#from db import connect\n",
    "#engine = connect()\n",
    "#kb_df.to_sql(name='kb_wiki_research_l_text',con=engine,if_exists='replace',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# triple中の代名詞youの候補の辞書を作るために，Greetingsの行からYouの候補を取り出す．\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "you = {}\n",
    "for mail in bodies['Bodies']:\n",
    "    idx = mail['idx']\n",
    "    for greetings in mail['Greetings']:\n",
    "        doc = nlp(greetings[0])\n",
    "        you[idx] = [(X.text, X.label_) for X in doc.ents if X.label_ not in ['DATE','TIME','PERCENT','MONEY','QUANTITY','ORDINAL','CARDINAL']]\n",
    "        if you[idx] == []:\n",
    "            del you[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tripleのI, MY, ME, WE, OUR, USの代名詞をSenderに置き換えるための辞書\n",
    "refer_you = {}\n",
    "for idx, candidate in you.items():\n",
    "    message_id = mail_df[\"Message-ID\"][idx]\n",
    "    refer_you[message_id] = {\"YOU\":candidate[0][0], \"YOUR\":candidate[0][0] + '\\'s'}\n",
    "\n",
    "# tripleのI, MY, ME, WE, OUR, USの代名詞をSenderに置き換えるための辞書\n",
    "refer_pronoun = {}\n",
    "for row in kb_df.values:\n",
    "    message_id = row[0]\n",
    "    sender = mail_df[mail_df['Message-ID']==message_id]['From'].values[0]\n",
    "    start = re.search(r'(\\(.+\\))',sender).start()\n",
    "    end = re.search(r'(\\(.+\\))',sender).end()\n",
    "    sender = sender[start+1:end-1]\n",
    "    refer_pronoun[message_id]={'I':sender, 'MY':sender + '\\'s', 'ME':sender, 'WE':sender, 'OUR':sender + '\\'s', 'US':sender}\n",
    "\n",
    "# 2つの辞書を結合する\n",
    "for key, value in refer_you.items():\n",
    "    if key in refer_pronoun:\n",
    "        d = refer_pronoun[key]\n",
    "        d.update(value)\n",
    "        refer_pronoun[key] = d\n",
    "    else:\n",
    "        refer_pronoun[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Senderに置き換えるための辞書を使って実際に置き換える\n",
    "replaced_rows = []\n",
    "for row in kb_df.values:\n",
    "    message_id = row[0]\n",
    "    arg1 = row[2].split()\n",
    "    rel = row[3].split()\n",
    "    arg2s = row[4].split()\n",
    "    replaced = []\n",
    "    # arg1の置き換え\n",
    "    for i, word in enumerate(arg1):\n",
    "        sender = refer_pronoun[message_id].get(word.upper())\n",
    "        if sender is None:\n",
    "            continue\n",
    "        else:\n",
    "            arg1[i] = sender\n",
    "    new_arg1 = ' '.join(arg1)\n",
    "    replaced.append(new_arg1)\n",
    "    # relの置き換え\n",
    "    for i, word in enumerate(rel):\n",
    "        sender = refer_pronoun[message_id].get(word.upper())\n",
    "        if sender is None:\n",
    "            continue\n",
    "        else:\n",
    "            rel[i] = sender\n",
    "    new_rel = ' '.join(rel)\n",
    "    replaced.append(new_rel)\n",
    "    # arg2sの置き換え\n",
    "    for i, word in enumerate(arg2s):\n",
    "        sender = refer_pronoun[message_id].get(word.upper())\n",
    "        if sender is None:\n",
    "            continue\n",
    "        else:\n",
    "            arg2s[i] = sender\n",
    "    new_arg2s = ' '.join(arg2s)\n",
    "    replaced.append(new_arg2s)\n",
    "    # [new_arg1, new_rel, new_arg2s]を1行として追加\n",
    "    replaced_rows.append(replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_triples = pd.DataFrame(replaced_rows,columns=['new_arg1','new_rel','new_arg2s'])\n",
    "kb_df = pd.concat([kb_df, replaced_triples],axis=1)\n",
    "kb_df.to_csv('/Users/taroaso/myprojects/OpenIE/wiki-research-l/output/full_replaced_triple_from_text_part.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from entityLinking import tagme, confidentAnnotations, mediaWiki\n",
    "\n",
    "entity_list = []\n",
    "for i, sentence in sentence_df['sentence'].items():\n",
    "    json_res = tagme(sentence)\n",
    "    for candidate in json_res['annotations']:\n",
    "        if candidate['rho'] >= 0.3:\n",
    "            entity_list.append([i, candidate['spot'],candidate['rho'],candidate['id'],candidate['title']])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# 抽出したentityをdataframeに格納する\n",
    "rows = []\n",
    "for row in entity_list:\n",
    "    rows.append([sentence_df['Message-ID'][row[0]], row[1], row[2], row[3], row[4]])\n",
    "entity_df = pd.DataFrame(rows,columns=['Message-ID','spot','rho','id','title'])\n",
    "entity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entity_df = pd.read_csv('wiki-research-l/output/entity_df.csv', index_col=0)\n",
    "entity_df[entity_df['rho'] < 0.3][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extraction import entityExtraction\n",
    "\n",
    "ner_list = []\n",
    "for i, sentence in sentence_df['sentence'].items():\n",
    "    entities = entityExtraction(sentence)\n",
    "    ner_list.append(entities)\n",
    "len(ner_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}