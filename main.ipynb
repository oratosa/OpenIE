{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys, email\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.metrics import *\n",
    "import pprint, re, time\n",
    "\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-10-05 16:01:05 INFO: Loading these models for language: en (English):\n=========================\n| Processor | Package   |\n-------------------------\n| tokenize  | ewt       |\n| pos       | ewt       |\n| lemma     | ewt       |\n| depparse  | ewt       |\n| sentiment | sstplus   |\n| ner       | ontonotes |\n=========================\n\n2020-10-05 16:01:05 INFO: Use device: cpu\n2020-10-05 16:01:05 INFO: Loading: tokenize\n2020-10-05 16:01:05 INFO: Loading: pos\n2020-10-05 16:01:07 INFO: Loading: lemma\n2020-10-05 16:01:07 INFO: Loading: depparse\n2020-10-05 16:01:09 INFO: Loading: sentiment\n2020-10-05 16:01:10 INFO: Loading: ner\n2020-10-05 16:01:11 INFO: Done loading processors!\n"
    }
   ],
   "source": [
    "### 自然言語処理\n",
    "import stanza\n",
    "#stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メールファイルからmail_dfを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loadFile import getFileList\n",
    "\n",
    "# ディレクトリ 内のメールファイルを読み込む\n",
    "directory_path = \"/Users/taroaso/myprojects/OpenIE/trec/2005/each_dataset/3\"\n",
    "file_list = getFileList(directory_path)\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "### mail_df \n",
    "\n",
    "mail_cols = ['docno','received','isoreceived','sent','isosent','name','email','subject','id','charset','inreplyto','expires','to','cc','body']\n",
    "mail_df = pd.DataFrame(index=[], columns=mail_cols)\n",
    "\n",
    "for file in file_list:\n",
    "    with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        mail = f.readlines()\n",
    "    \n",
    "        record={}\n",
    "        body = []\n",
    "        for row in mail:\n",
    "            if row.startswith('docno='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['docno'] = match.group().strip('\"')\n",
    "            elif row.startswith('received='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['received'] = match.group().strip('\"')\n",
    "            elif row.startswith('isoreceived='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['isoreceived'] = match.group().strip('\"')\n",
    "            elif row.startswith('sent='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['sent'] = match.group().strip('\"')\n",
    "            elif row.startswith('isosent='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['isosent'] = match.group().strip('\"')\n",
    "            elif row.startswith('name='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['name'] = match.group().strip('\"')\n",
    "            elif row.startswith('email='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['email'] = match.group().strip('\"')\n",
    "            elif row.startswith('subject='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['subject'] = match.group().strip('\"')\n",
    "            elif row.startswith('id='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['id'] = match.group().strip('\"')\n",
    "            elif row.startswith('charset='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['charset'] = match.group().strip('\"')\n",
    "            elif row.startswith('inreplyto='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['inreplyto'] = match.group().strip('\"')\n",
    "            elif row.startswith('expires='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['expires'] = match.group().strip('\"')\n",
    "            elif row.startswith('To:'):\n",
    "                match = row[3:-1]\n",
    "                record.setdefault('to',match)\n",
    "            elif row.startswith('Cc:'):\n",
    "                match = row[3:-1]\n",
    "                record.setdefault('cc',match)\n",
    "            else:\n",
    "                body.append(row)\n",
    "        record['body'] = '\\n'.join(body)\n",
    "    \n",
    "    mail_df = mail_df.append(record, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDBにmailテーブルを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RDBにmail_dfのテーブルを作成する\n",
    "from db import connect\n",
    "engine = connect()\n",
    "mail_df.to_sql(name='mail_3',con=engine,if_exists='replace',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下の2つに関する処理\n",
    "  1. メンションとEntityを対応させる辞書をつくる（entity_dict）\n",
    "  2. Entityテーブルのタプルとなるentity_rowsを作る（属性：message_id, entity）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mail_dfのヘッダー（from, date）から，メールアドレス，送信者，送受信日を取得する\n",
    "\n",
    "entity_rows = [] #Entityテーブルの行を格納するリスト\n",
    "entity_dict = {} \n",
    "for values in mail_df.values:\n",
    "    message_id = values[0]\n",
    "    date = values[3]\n",
    "    sender = values[5] # 送信者\n",
    "    address = values[6] # 送信者のメールアドレス\n",
    "    date = re.search(r'(\\d{1,2} \\w{3} \\d{2,4})', date) # 送信日\n",
    "    date = date.group()\n",
    "    # 辞書に登録する\n",
    "    entity_dict.setdefault(address,{address:\"MAIL\"})\n",
    "    entity_dict.setdefault(sender,{sender:\"PERSON\"})\n",
    "    entity_dict.setdefault(date,{date:\"DATE\"})\n",
    "    # 行に追加する\n",
    "    entity_rows.append((message_id, address))\n",
    "    entity_rows.append((message_id, sender))\n",
    "    entity_rows.append((message_id, date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bodyに含まれる文をEntity linkerにかけてEntityを取得する\n",
    "# entityLinkingFromBody.ipynb"
   ]
  },
  {
   "source": [
    "# mail_dfのsubjectをEntity linkerにかけてEntityを取得する．\n",
    "total_start = time.time()\n",
    "count = 0\n",
    "\n",
    "for values in mail_df.values:\n",
    "    subject = values[6]\n",
    "    subject = re.sub(r'(\\[.+\\] )','',subject)\n",
    "    subject = re.sub(r'(\\n\\t)',' ',subject)\n",
    "    subject = re.sub(r'\\n{1,}',' ',subject)\n",
    "    subject = re.sub(r'\\t{1,}',' ',subject)\n",
    "    \n",
    "    part_start = time.time()\n",
    "\n",
    "    json_res = tagme(subject)\n",
    "    linked_entities = [annotation for annotation in json_res['annotations'] if annotation['rho'] > 0.3]\n",
    "    if linked_entities != []:\n",
    "        for le in linked_entities:\n",
    "            spot = le['spot']\n",
    "            entity_dict.setdefault(spot,{le['title']:le['id']}) # 辞書に登録する\n",
    "            entity_rows.append((values[1],le['title'])) # 行に追加する\n",
    "\n",
    "    count = count + 1\n",
    "    print(\"Processing:{}%\".format((count/len(bodies)) * 100))\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(\"Total time:{} minutes\".format(total_time/60))"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "source": [
    "import pickle\n",
    "\n",
    "with open('/Users/taroaso/myprojects/OpenIE/trec/output/entity_dict.pickle', mode='wb') as f:\n",
    "    pickle.dump(entity_dict,f)\n",
    "\n",
    "with open('/Users/taroaso/myprojects/OpenIE/trec/output/entity_rows.pickle', mode='wb') as f:\n",
    "    pickle.dump(entity_rows,f)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新規Entity候補の抽出\n",
    "  * NERによって，Named Entityを抽出し，entity_dictに登録がなければ，新規Entity候補として，別の辞書に登録する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Entity_dictに登録のない新規エンティティ候補数:212\n"
    }
   ],
   "source": [
    "# 新規エンティティ候補を登録する辞書をentity_candidate_dictとし，行をentity_candidate_rowsとする\n",
    "entity_candidate_dict = {}\n",
    "entity_candidate_rows = []\n",
    "for mail in mail_df.values:\n",
    "    body = mail[-1]\n",
    "    #body = re.sub(r'\\n{2,}','\\n',mail['body'])\n",
    "    #body = re.sub(r'\\n{1}',' ',body)\n",
    "    #body = re.sub(r'( >){1,}','',body)\n",
    "    #body = re.sub(r'\\*{1,}','',body)\n",
    "    #body = re.sub(r'(On)','. On',body)\n",
    "    doc = nlp(body)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ not in ('DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'CARDINAL', 'ORDINAL'):\n",
    "            ent = ent.to_dict()\n",
    "            if ent[\"text\"] in list(entity_dict.keys()):\n",
    "                pass\n",
    "            else:\n",
    "                entity_candidate_dict[ent[\"text\"]] = ent[\"type\"]\n",
    "                entity_candidate_rows.append((mail['message_id'],ent[\"text\"]))\n",
    "print('Entity_dictに登録のない新規エンティティ候補数:{}'.format(len(entity_candidate_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新規エンティティ候補（メンション）をentity_dictに登録されているメンションと名寄せする（類似のメンションを検出する）\n",
    "* 名寄せできたメンションとエンティティの辞書をつくる（add_entity_dict）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "entity_dictと名寄せできたメンション数:43\n"
    }
   ],
   "source": [
    "# 名寄せ\n",
    "add_entity_dict = {}\n",
    "\n",
    "for candidate in entity_candidate_dict.keys():\n",
    "    threshold = 0.5\n",
    "    candidate_tokens = candidate.split()\n",
    "    filtered_candidate_tokens = [token for token in candidate_tokens if token not in stop_words]\n",
    "    filtered_candidate_tokens = set(list(map(str.lower, filtered_candidate_tokens)))\n",
    "    for spot in entity_dict.keys():\n",
    "        # jaccard distance\n",
    "        spot_tokens= spot.split()\n",
    "        filtered_spot_tokens = [token for token in spot_tokens if token not in stop_words]\n",
    "        filtered_spot_tokens = set(list(map(str.lower,filtered_spot_tokens)))\n",
    "        jd = jaccard_distance(filtered_candidate_tokens, filtered_spot_tokens)\n",
    "        # edit distance\n",
    "        filtered_spot = ' '.join(filtered_spot_tokens)\n",
    "        filtered_candidate = ' '.join(filtered_candidate_tokens)\n",
    "        ed = edit_distance(filtered_spot, filtered_candidate)/max(len(filtered_spot),len(filtered_candidate))\n",
    "        if min(jd,ed) < threshold:\n",
    "            threshold = min(jd,ed)\n",
    "            add_entity_dict[candidate] = entity_dict[spot]\n",
    "print('entity_dictと名寄せできたメンション数:{}'.format(len(add_entity_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新規エンティティ候補内で名寄せを行い，その中の1つにマッピングする\n",
    "* マッピングした辞書をmapped_ne_dictとする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entity_candidate_list = list(entity_candidate_dict.keys() - add_entity_dict.keys())\n",
    "entity_candidate_list_lower = list(map(str.lower, entity_candidate_list))\n",
    "similar_dict = {}\n",
    "for i, ne1 in enumerate(entity_candidate_list_lower):\n",
    "    ne1_tokens = ne1.split()\n",
    "    ne1_tokens = [token for token in ne1_tokens if token not in stop_words]\n",
    "    ne1_strings = ' '.join(ne1_tokens)\n",
    "    similar_ne = [] # 類似するentity candidateをまとめる\n",
    "    for j, ne2 in enumerate(entity_candidate_list_lower):\n",
    "        # jaccard distance\n",
    "        ne2_tokens = ne2.split()\n",
    "        ne2_tokens = [token for token in ne2_tokens if token not in stop_words]\n",
    "        jd = jaccard_distance(set(ne1_tokens), set(ne2_tokens))\n",
    "        # edit distance\n",
    "        ne2_strings = ' '.join(ne2_tokens)\n",
    "        ed = edit_distance(ne1_strings, ne2_strings)/max(len(ne1_strings),len(ne2_strings))\n",
    "        if min(jd,ed) < 0.4:\n",
    "            similar_ne.append(entity_candidate_list[j])\n",
    "    similar_dict[entity_candidate_list[i]] = similar_ne\n",
    "\n",
    "mapped_ne_dict = {}\n",
    "for key, value_list in similar_dict.items():\n",
    "    length_list = list(map(lambda x:len(x), value_list))\n",
    "    idx = length_list.index(min(length_list))\n",
    "    mapped_ne_dict[key] = {value_list[idx]:entity_candidate_dict[value_list[idx]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3つの辞書（entity_dict, add_entity_dict, mapped_ne_dict）を統合する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "integrated_entity_dict = dict(**entity_dict,**add_entity_dict,**mapped_ne_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wiki-research-l/output/integrated_entity_dict.pkl\",\"wb\") as f:\n",
    "    pickle.dump(integrated_entity_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"wiki-research-l/output/integrated_entity_dict.pkl\",\"rb\") as f:\n",
    "    integrated_entity_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entity_candidate_rowsの新規エンティティ候補（メンション）を辞書と照合し，対応するエンティティと置き換え，entity_rowsに追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in entity_candidate_rows:\n",
    "    mention = row[1]\n",
    "    entity = list(integrated_entity_dict[mention].keys())[0]\n",
    "    row = (row[0], entity)\n",
    "    entity_rows.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDBにEntityテーブルを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDBにmail_dfのテーブルを作成する\n",
    "entity_df = pd.DataFrame(entity_rows, columns=['message_id','entity'])\n",
    "\n",
    "from db import connect\n",
    "engine = connect()\n",
    "\n",
    "entity_df.to_sql(name='wiki_research_l_entity',con=engine,if_exists='replace',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 代名詞YOUに関する辞書\n",
    "refer_you = {}\n",
    "for mail in bodies:\n",
    "    message_id = mail['message_id']\n",
    "    # triple中の代名詞youの候補の辞書を作るために，Greetingsの行からYouの候補を取り出す．\n",
    "    for greetings in mail['greetings']:\n",
    "        if greetings != []:\n",
    "            doc = nlp(greetings[0])\n",
    "            for ent in doc.ents:\n",
    "                if ent.type in ['PERSON']:\n",
    "                    refer_you[message_id] = {\"YOU\":ent.text, \"YOUR\":ent.text + '\\'s'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# その他の代名詞I, MY, ME, WE, OUR, USに関する辞書\n",
    "refer_pronoun = {}\n",
    "for values in mail_df.values:\n",
    "    message_id = values[1]\n",
    "    sender = values[2]\n",
    "    start = re.search(r'(\\(.+\\))',sender).start()\n",
    "    end = re.search(r'(\\(.+\\))',sender).end()\n",
    "    sender = sender[start+1:end-1]\n",
    "    refer_pronoun[message_id]={'I':sender, 'MY':sender + '\\'s', 'ME':sender, 'WE':sender, 'OUR':sender + '\\'s', 'US':sender}\n",
    "\n",
    "# 2つの辞書を結合する\n",
    "for key, value in refer_you.items():\n",
    "    if key in refer_pronoun:\n",
    "        d = refer_pronoun[key]\n",
    "        d.update(value)\n",
    "        refer_pronoun[key] = d\n",
    "    else:\n",
    "        refer_pronoun[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文章からトリプルを抽出する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence_list = []\n",
    "for mail in bodies:  #1通ずつ取り出す\n",
    "    text = ''\n",
    "    # 1行{行番号:文}ずつ取り出し，複数文が含まれた1つの文の連なりにする\n",
    "    for sentence in mail['sentence']: \n",
    "        text = text + list(sentence.values())[0] + ' '\n",
    "    # 文章を文に分解する\n",
    "    if text == '':\n",
    "        pass\n",
    "    else:\n",
    "        doc = nlp(text)\n",
    "        for sentence in doc.sentences:\n",
    "            # 単語のlemmatizationを辞書にする\n",
    "            lemma = {}\n",
    "            for word in sentence.words:\n",
    "                lemma[word.text] = word.lemma\n",
    "            sentence_list.append((mail['message_id'], sentence.text, lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total time:0.5135371287663778 minutes\n"
    }
   ],
   "source": [
    "total_start = time.time()\n",
    "# MinIEにかける\n",
    "import requests\n",
    "import json\n",
    "\n",
    "extractions_list = []\n",
    "for tpl in sentence_list:\n",
    "    message_id = tpl[0]\n",
    "    sentence = tpl[1].encode('utf-8')\n",
    "    lemma = tpl[-1]\n",
    "    try:\n",
    "        response = requests.post('http://localhost:8080/minie/query', data=sentence)\n",
    "        result = response.json()\n",
    "        if result['facts'] == []:\n",
    "            pass\n",
    "        else:\n",
    "            for triple in result['facts']:\n",
    "                sbj = triple['subject']\n",
    "                obj = triple['object']\n",
    "                lemma_predicate = triple['predicate']\n",
    "#                token_list = triple['predicate'].split()\n",
    "#                lemma_list = []\n",
    "#                for token in token_list:\n",
    "#                    try:\n",
    "#                        lemma_list.append(lemma[token])\n",
    "#                    except:\n",
    "#                        lemma_list.append(token)\n",
    "#                lemma_predicate = ' '.join(lemma_list)\n",
    "                extractions_list.append([message_id, sentence.decode('utf-8'), sbj, lemma_predicate, obj])\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "# dataframeにする\n",
    "triple_df = pd.DataFrame(extractions_list, columns = ['message_id','sentence', 'subject', 'predicate', 'object'])\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(\"Total time:{} minutes\".format(total_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トリプルを代名詞の辞書と照合し，代名詞が含まれていれば対応するエンティティで置き換える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Senderに置き換えるための辞書を使って実際に置き換える\n",
    "triples = []\n",
    "for row in triple_df.values:\n",
    "    message_id = row[0]\n",
    "    sentence = row[1]\n",
    "    sbj = row[2].split()\n",
    "    pred = row[3].split()\n",
    "    obj = row[4].split()\n",
    "    pronouns = refer_pronoun[message_id]\n",
    "    # subjectの置き換え\n",
    "    for i, word in enumerate(sbj):\n",
    "        entity = pronouns.get(word.upper())\n",
    "        if entity is None:\n",
    "            continue\n",
    "        else:\n",
    "            sbj[i] = entity\n",
    "    sbj = ' '.join(sbj)\n",
    "    # predicateの置き換え\n",
    "    for i, word in enumerate(pred):\n",
    "        entity = pronouns.get(word.upper())\n",
    "        if entity is None:\n",
    "            continue\n",
    "        else:\n",
    "            pred[i] = entity\n",
    "    pred = ' '.join(pred)\n",
    "    # objectの置き換え\n",
    "    for i, word in enumerate(obj):\n",
    "        entity = pronouns.get(word.upper())\n",
    "        if entity is None:\n",
    "            continue\n",
    "        else:\n",
    "            obj[i] = entity\n",
    "    obj = ' '.join(obj)\n",
    "    # [new_arg1, new_rel, new_arg2s]を1行として追加\n",
    "    triples.append((message_id, sentence, sbj, pred, obj))"
   ]
  },
  {
   "source": [
    "# トリプルの主語，目的語が代名詞のみの場合，そのトリプルを除外（削除）する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# トリプルの述語に\"...\"が含まれている場合，そのトリプルを削除する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# トリプルの述語の一部をlemmatizationする"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トリプルとEntityの辞書を照合し，辞書に登録されているEntityで置き換える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_triples = []\n",
    "mentions = list(integrated_entity_dict.keys())\n",
    "for row in triples:\n",
    "    message_id = row[0]\n",
    "    sentence = row[1]\n",
    "    sbj = row[2]\n",
    "    pred = row[3]\n",
    "    obj = row[4]\n",
    "\n",
    "    # subjectあるいはobjectが辞書に登録されているメンションと一致する場合\n",
    "    if sbj in mentions:\n",
    "        sbj = list(integrated_entity_dict[sbj].keys()) #メンションに対応するEntityで置き換える\n",
    "        sbj = sbj[0]\n",
    "        canonical_triples.append((message_id, sentence, sbj, pred, obj))\n",
    "    elif obj in mentions:\n",
    "        obj = list(integrated_entity_dict[obj].keys()) #メンションに対応するEntityで置き換える\n",
    "        obj = obj[0]\n",
    "        canonical_triples.append((message_id, sentence, sbj, pred, obj))\n",
    "    else:\n",
    "        for mention in mentions:\n",
    "            entity = list(integrated_entity_dict[mention].keys())\n",
    "            entity = entity[0]\n",
    "            if re.search(re.escape(mention), sbj):\n",
    "                canonical_triples.append((message_id, sentence, sbj, pred, obj))\n",
    "                canonical_triples.append((message_id, sentence, entity, 'seeAlso', sbj))\n",
    "            elif re.search(re.escape(mention), obj):\n",
    "                canonical_triples.append((message_id, sentence, sbj, pred, obj))\n",
    "                canonical_triples.append((message_id, sentence, entity, 'seeAlso', obj))\n",
    "\n",
    "canonical_triple_df = pd.DataFrame(canonical_triples, columns=['message_id', 'sentence', 'sbject', 'predicate', 'object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDBにtripleテーブルを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db import connect\n",
    "engine = connect()\n",
    "\n",
    "canonical_triple_df = canonical_triple_df.drop_duplicates()\n",
    "canonical_triple_df.to_sql(name='wiki_research_l_triple',con=engine,if_exists='replace',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['has', 'be great suggestion to include', 'seeAlso', 'be include',\n       'be familiar with', 'love to interview you for',\n       'love test visualization for', 'love to interview', 'love',\n       'schedule', 'be look for researcher so unfortunately in',\n       'be look for developer so unfortunately in', 'be look for', 'be',\n       'be to bring together',\n       'be to bring together QUANT_O_1 party work toward',\n       'invite submission of', 'talk',\n       'organize experience Wikipedians to thank thousand of',\n       'find that receive in', 'find that',\n       'increase QUANT_R_1 week retention by',\n       'increase QUANT_R_1 week retention on', 'increase', 'look at',\n       'do find', 'feel', 'has effort on',\n       'make valuable discovery about who spend time support other because of',\n       'value', 'move pre-prints toward submission for', 'move', 'is',\n       'has team of', 'be grateful to',\n       'partner with J. Nathan Matias in', 'partner with', 'has in',\n       'work with', 'work in design of', 'work',\n       'be do in collaboration with', 'be co-create by',\n       'be work to open process & software to wider range of community in',\n       'be work to open process & software to wider range of researcher in',\n       'be work to open process & software in', 'be work to open',\n       'be building on workshop QUANT_O_1 > in',\n       'be building on community research summit QUANT_O_1 > in',\n       'be develop idea for next round of collaboration toward',\n       'be develop fundraising toward', 'be develop',\n       'be develop idea for next round of collaboration',\n       'be develop fundraising', 'has next round of',\n       'have start new Formal collaboration with',\n       'have start new Formal collaboration with University of Turin on',\n       'is in', 'have engagement with Images in',\n       'be main formal collaborator', 'be thankful to', 'be thankful for',\n       'aim to keep', 'link to', 'be capture', 'be point of',\n       'be neuroscience researcher at',\n       'be interested in explore biase on', 'hope to analyze', 'identify',\n       'to see actually', 'to see', 'need to be', 'do intend to request',\n       'be live-streamed on Wednesday at', 'be live-streamed on',\n       'be QUANT_R_1 of most important online resource for',\n       'have be highlight during', 'have be create',\n       'get better understanding of state of medical knowledge in',\n       \"give overview on how Wikipedia 's health content be use by different audience in\",\n       'be use by', 'join conversation on irc at',\n       'be most access web site for', 'engage with', 'have',\n       'be QUANT_R_1 of', 'have be publish online at', 'have be publish',\n       'be base on', 'to rely on',\n       'assess coverage of COVID-19-related research in Wikipedia via',\n       'assess', 'be integrate new research at', 'be integrate',\n       'be represent from', 'figure out', 'regretfully do have',\n       'be target', 'suggest', '-LSB- send -RSB- detail to',\n       '-LSB- send -RSB- directly', '-LSB- send -RSB-', 'post detail to',\n       'post', 'look to read experimental design take factor into',\n       'look',\n       'have article on QUANT_R_1 female fellow of Royal Society for',\n       'achieve', 'have article on QUANT_R_1 Nobel Prize Winner by',\n       'have article on', 'be ask to restore', 'tell they to start from',\n       'have develop to investigate computer base system for',\n       'have develop', 'be register for', 'be available under', 'be in',\n       'reach out to legal to inquire about',\n       'reach out at wikimedia.org to inquire about',\n       'reach out to inquire about', 'try to parse', 'need to go back in',\n       'need to go back longer than', 'need to go back as', 'need to go',\n       'be list at', 'show you next set of', 'prefer', 'have look at',\n       'cover', 'have have have take place', 'be geolocate',\n       'be geolocate ip address in', 'be QUANT_O_1 most important for',\n       'be QUANT_R_1 of deletion process QUANT_O_1 most important for',\n       'be QUANT_R_1 day process for',\n       'be surprised if article be delete per A7 that',\n       'be surprised that', 'have be delete per', 'expect that to find',\n       'be happen on', 'be available knowledge base host by',\n       'be host by',\n       'have see increase in number of publication around Wikidata in',\n       'have see', 'hope to provide', 'address challenge of',\n       'encourage range of', 'encourage', 'encourage description of',\n       'be link to', \"'re\",\n       'feed for instance by improve on QUANT_R_1 wikidata aspect back into',\n       'feed for instance by comment back into',\n       'feed for instance by suggest new design feature back into',\n       'feed for instance by suggest tool back into',\n       'feed for instance by suggest practice back into',\n       'encourage submission on', 'is natural language generation by',\n       'is abstract representation of', 'welcome', 'discuss area of',\n       'shed light on benefit of', 'be consider online option in',\n       'be consider', 'welcome follow type of', 'be as',\n       \"'m with researcher at\", 'be work on identify missing content on',\n       'speak', 'be compensate for', 'be work on', 'be work as part of',\n       'be work', 'have start',\n       'have start new Formal collaboration with team from École Polytechnique Fédérale de Lausanne to work collaboratively on',\n       'contribute to program', 'contribute', 'be to those of you to',\n       'be to those of you for', 'be to those of', 'continue',\n       'have work with', 'be shape proposal for', 'act as',\n       'be program approach to analyze',\n       'be perform by ip address out of', 'be perform by', 'be miss',\n       'be aware of', 'map wikidata qid to', 'map', 'have be use',\n       'write up quickly', 'publish', 'write up', 'need to generate',\n       'do read response', 'do read', 'invite',\n       'teach you most unexpected thing about', 'be new to', 'learn',\n       'have parse', 'make dev table publicly available on',\n       'be commit to', \"'re prepare for\", 'look for',\n       'be rise ... it give good model for decline editor number for',\n       'be decline ... it give good model for decline editor number for',\n       'be rise ... it give personally good model for',\n       'be decline ... it give personally good model for',\n       'be rise ... it give good model for',\n       'be decline ... it give good model for',\n       'it be ... it give good model for decline editor number for',\n       'it be ... it give personally good model for',\n       'it be ... it give good model for',\n       'it be rise ... give good model for decline editor number for',\n       'it be decline ... give good model for decline editor number for',\n       'it be rise ... give personally good model for',\n       'it be decline ... give personally good model for',\n       'it be rise ... give good model for',\n       'it be decline ... give good model for', 'have be influential in',\n       'have be', 'view Wikipedia', 'view'], dtype=object)"
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "source": [
    "canonical_triple_df['predicate'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}