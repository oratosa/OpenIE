{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys, email\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.metrics import *\n",
    "import pprint, re, time\n",
    "\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-09-14 16:09:31 INFO: Loading these models for language: en (English):\n=========================\n| Processor | Package   |\n-------------------------\n| tokenize  | ewt       |\n| pos       | ewt       |\n| lemma     | ewt       |\n| depparse  | ewt       |\n| sentiment | sstplus   |\n| ner       | ontonotes |\n=========================\n\n2020-09-14 16:09:31 INFO: Use device: cpu\n2020-09-14 16:09:31 INFO: Loading: tokenize\n2020-09-14 16:09:31 INFO: Loading: pos\n2020-09-14 16:09:32 INFO: Loading: lemma\n2020-09-14 16:09:32 INFO: Loading: depparse\n2020-09-14 16:09:33 INFO: Loading: sentiment\n2020-09-14 16:09:35 INFO: Loading: ner\n2020-09-14 16:09:35 INFO: Done loading processors!\n"
    }
   ],
   "source": [
    "### 自然言語処理\n",
    "import stanza\n",
    "#stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メールファイルからmail_dfを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### mail_df \n",
    "\n",
    "from loadFile import getFileList\n",
    "\n",
    "# ディレクトリ 内のメールファイルを読み込む\n",
    "directory_path = \"wiki-research-l/2020-July\"\n",
    "file_list = getFileList(directory_path)\n",
    "file_list.sort()\n",
    "\n",
    "# テキストファイルをデータフレームに格納する\n",
    "\n",
    "from email.parser import Parser\n",
    "\n",
    "mail_cols = ['file_path','message_id','from','date','in_reply_to','references','subject','body']\n",
    "mail_df = pd.DataFrame(index=[], columns=mail_cols)\n",
    "    \n",
    "for file in file_list:\n",
    "    with open(file) as f:\n",
    "        mail = Parser().parse(f)\n",
    "\n",
    "    record = {}\n",
    "    for col in mail_cols:\n",
    "        if col == 'file_path':\n",
    "            record[col] = file\n",
    "        elif col == 'message_id':\n",
    "            record[col] = mail.get('Message-ID')\n",
    "        elif col == 'from':\n",
    "            record[col] = mail.get('From')\n",
    "        elif col == 'date':\n",
    "            record[col] = mail.get('Date')\n",
    "        elif col == 'in_reply_to':\n",
    "            record[col] = mail.get('In-Reply-To')\n",
    "        elif col == 'references':\n",
    "            record[col] = mail.get('References')\n",
    "        elif col == 'subject':\n",
    "            record[col] = mail.get('Subject')\n",
    "        elif col == 'body':\n",
    "            record[col] = mail.get_payload()\n",
    "            \n",
    "    mail_df = mail_df.append(record, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDBにmailテーブルを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RDBにmail_dfのテーブルを作成する\n",
    "from db import connect\n",
    "engine = connect()\n",
    "mail_df.to_sql(name='wiki_research_l_mail',con=engine,if_exists='replace',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メールのBody部分の前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メールのBody部分を各パートに分解する\n",
    "bodies = []\n",
    "for idx, body in mail_df.loc[:,'body'].items():\n",
    "    origin = []\n",
    "    greetings = dict(greetings=[])\n",
    "    sentences = dict(sentence=[])\n",
    "    captions = dict(caption=[])\n",
    "    bulletlist = dict(bulletlist=[])\n",
    "    ending = dict(ending=[])\n",
    "    quotation = dict(quotation=[])\n",
    "    footer = dict(footer=[])\n",
    "    misc = dict(misc=[])\n",
    "\n",
    "    lines = body.splitlines()\n",
    "    for num, line in enumerate(lines):\n",
    "        if re.match(r'\\[G\\]',line) is not None:\n",
    "            greetings['greetings'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[S\\]',line) is not None:\n",
    "            sentences['sentence'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[C\\]',line) is not None:\n",
    "            captions['caption'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[B\\]',line) is not None:\n",
    "            bulletlist['bulletlist'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[E\\]',line) is not None:\n",
    "            ending['ending'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[Q\\]',line) is not None:\n",
    "            quotation['quotation'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[F\\]',line) is not None:\n",
    "            footer['footer'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[M\\]',line) is not None:\n",
    "            misc['misc'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        else: #空白行に対応する\n",
    "            continue\n",
    "    originbody = re.sub(r'\\[(G|S|C|B|E|Q|F|M)\\]', '', body)\n",
    "    bodies.append({'idx':idx, 'message_id':mail_df['message_id'][idx], 'countrows':len(lines), 'body':originbody, **greetings, **sentences, **captions, **bulletlist, **ending, **quotation, **footer, **misc})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下の2つに関する処理\n",
    "  1. メンションとEntityを対応させる辞書をつくる（entity_dict）\n",
    "  2. Entityテーブルのタプルとなるentity_rowsを作る（属性：message_id, entity）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mail_dfのヘッダー（from, date）から，メールアドレス，送信者，送受信日を取得する\n",
    "\n",
    "entity_rows = [] #Entityテーブルの行を格納するリスト\n",
    "entity_dict = {} \n",
    "for values in mail_df.values:\n",
    "    message_id = values[1]\n",
    "    date = values[3]\n",
    "    address_sender = values[2]\n",
    "    start = re.search(r'(\\(.+\\))', address_sender).start()\n",
    "    end = re.search(r'(\\(.+\\))', address_sender).end()\n",
    "    address = address_sender[0:start-1] # メールアドレス\n",
    "    sender = address_sender[start+1:end-1] # 送信者\n",
    "    date = re.search(r'(\\d{1,2} \\w{3} \\d{4})',date) # 送受信日\n",
    "    date = date.group()\n",
    "    # 辞書に登録する\n",
    "    entity_dict.setdefault(address,{address:\"MAIL\"})\n",
    "    entity_dict.setdefault(sender,{sender:\"PERSON\"})\n",
    "    entity_dict.setdefault(date,{date:\"DATE\"})\n",
    "    # 行に追加する\n",
    "    entity_rows.append((message_id, address))\n",
    "    entity_rows.append((message_id, sender))\n",
    "    entity_rows.append((message_id, date))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Processing:2.7027027027027026%\nProcessing:5.405405405405405%\nProcessing:8.108108108108109%\nProcessing:10.81081081081081%\nProcessing:13.513513513513514%\nProcessing:16.216216216216218%\nProcessing:18.91891891891892%\nProcessing:21.62162162162162%\nProcessing:24.324324324324326%\nProcessing:27.027027027027028%\nProcessing:29.72972972972973%\nProcessing:32.432432432432435%\nProcessing:35.13513513513514%\nProcessing:37.83783783783784%\nProcessing:40.54054054054054%\nProcessing:43.24324324324324%\nProcessing:45.94594594594595%\nProcessing:48.64864864864865%\nProcessing:51.35135135135135%\nProcessing:54.054054054054056%\nProcessing:56.75675675675676%\nProcessing:59.45945945945946%\nProcessing:62.16216216216216%\nProcessing:64.86486486486487%\nProcessing:67.56756756756756%\nProcessing:70.27027027027027%\nProcessing:72.97297297297297%\nProcessing:75.67567567567568%\nProcessing:78.37837837837837%\nProcessing:81.08108108108108%\nProcessing:83.78378378378379%\nProcessing:86.48648648648648%\nProcessing:89.1891891891892%\nProcessing:91.8918918918919%\nProcessing:94.5945945945946%\nProcessing:97.2972972972973%\nProcessing:100.0%\nTotal time:30.604731333255767 minutes\n"
    }
   ],
   "source": [
    "# Bodyに含まれる文をEntity linkerにかけてEntityを取得する．\n",
    "total_start = time.time()\n",
    "\n",
    "from entityLinking import tagme\n",
    "\n",
    "for mail in bodies:\n",
    "    body = re.sub(r'\\n{2,}','\\n',mail['body'])\n",
    "    body = re.sub(r'\\n{1}',' ',body)\n",
    "    body = re.sub(r'( >){1,}','',body)\n",
    "    body = re.sub(r'\\*{1,}','',body)\n",
    "    body = re.sub(r'(On)','. On',body)\n",
    "    doc = nlp(body)\n",
    "    \n",
    "    part_start = time.time()\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        json_res = tagme(sentence.text)\n",
    "        linked_entities = [annotation for annotation in json_res['annotations'] if annotation['rho'] > 0.3]\n",
    "        if linked_entities != []:\n",
    "            for le in linked_entities:\n",
    "                spot = le['spot']\n",
    "                entity_dict.setdefault(spot,{le['title']:le['id']}) # 辞書に登録する\n",
    "                entity_rows.append((mail['message_id'],le['title'])) # 行に追加する\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(\"Total time:{} minutes\".format(total_time/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "257 946\n"
    }
   ],
   "source": [
    "print(len(entity_dict),len(entity_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Processing:2.7027027027027026%\nProcessing:5.405405405405405%\nProcessing:8.108108108108109%\nProcessing:10.81081081081081%\nProcessing:13.513513513513514%\nProcessing:16.216216216216218%\nProcessing:18.91891891891892%\nProcessing:21.62162162162162%\nProcessing:24.324324324324326%\nProcessing:27.027027027027028%\nProcessing:29.72972972972973%\nProcessing:32.432432432432435%\nProcessing:35.13513513513514%\nProcessing:37.83783783783784%\nProcessing:40.54054054054054%\nProcessing:43.24324324324324%\nProcessing:45.94594594594595%\nProcessing:48.64864864864865%\nProcessing:51.35135135135135%\nProcessing:54.054054054054056%\nProcessing:56.75675675675676%\nProcessing:59.45945945945946%\nProcessing:62.16216216216216%\nProcessing:64.86486486486487%\nProcessing:67.56756756756756%\nProcessing:70.27027027027027%\nProcessing:72.97297297297297%\nProcessing:75.67567567567568%\nProcessing:78.37837837837837%\nProcessing:81.08108108108108%\nProcessing:83.78378378378379%\nProcessing:86.48648648648648%\nProcessing:89.1891891891892%\nProcessing:91.8918918918919%\nProcessing:94.5945945945946%\nProcessing:97.2972972972973%\nProcessing:100.0%\nTotal time:0.8869345982869467 minutes\n"
    }
   ],
   "source": [
    "# mail_dfのsubjectをEntity linkerにかけてEntityを取得する．\n",
    "total_start = time.time()\n",
    "count = 0\n",
    "\n",
    "for values in mail_df.values:\n",
    "    subject = values[6]\n",
    "    subject = re.sub(r'(\\[.+\\] )','',subject)\n",
    "    subject = re.sub(r'(\\n\\t)',' ',subject)\n",
    "    subject = re.sub(r'\\n{1,}',' ',subject)\n",
    "    subject = re.sub(r'\\t{1,}',' ',subject)\n",
    "    \n",
    "    part_start = time.time()\n",
    "\n",
    "    json_res = tagme(subject)\n",
    "    linked_entities = [annotation for annotation in json_res['annotations'] if annotation['rho'] > 0.3]\n",
    "    if linked_entities != []:\n",
    "        for le in linked_entities:\n",
    "            spot = le['spot']\n",
    "            entity_dict.setdefault(spot,{le['title']:le['id']}) # 辞書に登録する\n",
    "            entity_rows.append((values[1],le['title'])) # 行に追加する\n",
    "\n",
    "    count = count + 1\n",
    "    print(\"Processing:{}%\".format((count/len(bodies)) * 100))\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(\"Total time:{} minutes\".format(total_time/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"wiki-research-l/output/entity_dict.pkl\",\"wb\") as f:\n",
    "    pickle.dump(entity_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"wiki-research-l/output/entity_dict.pkl\", 'rb') as f:\n",
    "    entity_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新規Entity候補の抽出\n",
    "  * NERによって，Named Entityを抽出し，entity_dictに登録がなければ，新規Entity候補として，別の辞書に登録する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Entity_dictに登録のない新規エンティティ候補数:212\n"
    }
   ],
   "source": [
    "# 新規エンティティ候補を登録する辞書をentity_candidate_dictとし，行をentity_candidate_rowsとする\n",
    "entity_candidate_dict = {}\n",
    "entity_candidate_rows = []\n",
    "for mail in bodies:\n",
    "    body = re.sub(r'\\n{2,}','\\n',mail['body'])\n",
    "    body = re.sub(r'\\n{1}',' ',body)\n",
    "    body = re.sub(r'( >){1,}','',body)\n",
    "    body = re.sub(r'\\*{1,}','',body)\n",
    "    body = re.sub(r'(On)','. On',body)\n",
    "    doc = nlp(body)\n",
    "    for ent in doc.ents:\n",
    "        if ent.type not in ('DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'CARDINAL', 'ORDINAL'):\n",
    "            ent = ent.to_dict()\n",
    "            if ent[\"text\"] in list(entity_dict.keys()):\n",
    "                pass\n",
    "            else:\n",
    "                entity_candidate_dict[ent[\"text\"]] = ent[\"type\"]\n",
    "                entity_candidate_rows.append((mail['message_id'],ent[\"text\"]))\n",
    "print('Entity_dictに登録のない新規エンティティ候補数:{}'.format(len(entity_candidate_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wiki-research-l/output/ne_dict.pkl\",\"wb\") as f:\n",
    "    pickle.dump(ne_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新規エンティティ候補（メンション）をentity_dictに登録されているメンションと名寄せする（類似のメンションを検出する）\n",
    "* 名寄せできたメンションとエンティティの辞書をつくる（add_entity_dict）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "entity_dictと名寄せできたメンション数:43\n"
    }
   ],
   "source": [
    "# 名寄せ\n",
    "add_entity_dict = {}\n",
    "\n",
    "for candidate in entity_candidate_dict.keys():\n",
    "    threshold = 0.5\n",
    "    candidate_tokens = candidate.split()\n",
    "    filtered_candidate_tokens = [token for token in candidate_tokens if token not in stop_words]\n",
    "    filtered_candidate_tokens = set(list(map(str.lower, filtered_candidate_tokens)))\n",
    "    for spot in entity_dict.keys():\n",
    "        # jaccard distance\n",
    "        spot_tokens= spot.split()\n",
    "        filtered_spot_tokens = [token for token in spot_tokens if token not in stop_words]\n",
    "        filtered_spot_tokens = set(list(map(str.lower,filtered_spot_tokens)))\n",
    "        jd = jaccard_distance(filtered_candidate_tokens, filtered_spot_tokens)\n",
    "        # edit distance\n",
    "        filtered_spot = ' '.join(filtered_spot_tokens)\n",
    "        filtered_candidate = ' '.join(filtered_candidate_tokens)\n",
    "        ed = edit_distance(filtered_spot, filtered_candidate)/max(len(filtered_spot),len(filtered_candidate))\n",
    "        if min(jd,ed) < threshold:\n",
    "            threshold = min(jd,ed)\n",
    "            add_entity_dict[candidate] = entity_dict[spot]\n",
    "print('entity_dictと名寄せできたメンション数:{}'.format(len(add_entity_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新規エンティティ候補内で名寄せを行い，その中の1つにマッピングする\n",
    "* マッピングした辞書をmapped_ne_dictとする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entity_candidate_list = list(entity_candidate_dict.keys() - add_entity_dict.keys())\n",
    "entity_candidate_list_lower = list(map(str.lower, entity_candidate_list))\n",
    "similar_dict = {}\n",
    "for i, ne1 in enumerate(entity_candidate_list_lower):\n",
    "    ne1_tokens = ne1.split()\n",
    "    ne1_tokens = [token for token in ne1_tokens if token not in stop_words]\n",
    "    ne1_strings = ' '.join(ne1_tokens)\n",
    "    similar_ne = [] # 類似するentity candidateをまとめる\n",
    "    for j, ne2 in enumerate(entity_candidate_list_lower):\n",
    "        # jaccard distance\n",
    "        ne2_tokens = ne2.split()\n",
    "        ne2_tokens = [token for token in ne2_tokens if token not in stop_words]\n",
    "        jd = jaccard_distance(set(ne1_tokens), set(ne2_tokens))\n",
    "        # edit distance\n",
    "        ne2_strings = ' '.join(ne2_tokens)\n",
    "        ed = edit_distance(ne1_strings, ne2_strings)/max(len(ne1_strings),len(ne2_strings))\n",
    "        if min(jd,ed) < 0.4:\n",
    "            similar_ne.append(entity_candidate_list[j])\n",
    "    similar_dict[entity_candidate_list[i]] = similar_ne\n",
    "\n",
    "mapped_ne_dict = {}\n",
    "for key, value_list in similar_dict.items():\n",
    "    length_list = list(map(lambda x:len(x), value_list))\n",
    "    idx = length_list.index(min(length_list))\n",
    "    mapped_ne_dict[key] = {value_list[idx]:entity_candidate_dict[value_list[idx]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3つの辞書（entity_dict, add_entity_dict, mapped_ne_dict）を統合する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "integrated_entity_dict = dict(**entity_dict,**add_entity_dict,**mapped_ne_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entity_candidate_rowsの新規エンティティ候補（メンション）を辞書と照合し，対応するエンティティと置き換え，entity_rowsに追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in entity_candidate_rows:\n",
    "    mention = row[1]\n",
    "    entity = list(integrated_entity_dict[mention].keys())[0]\n",
    "    row = (row[0], entity)\n",
    "    entity_rows.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDBにEntityテーブルを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDBにmail_dfのテーブルを作成する\n",
    "entity_df = pd.DataFrame(entity_rows, columns=['message_id','entity'])\n",
    "\n",
    "from db import connect\n",
    "engine = connect()\n",
    "\n",
    "entity_df.to_sql(name='wiki_research_l_entity',con=engine,if_exists='replace',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 代名詞YOUに関する辞書\n",
    "refer_you = {}\n",
    "for mail in bodies:\n",
    "    message_id = mail['message_id']\n",
    "    # triple中の代名詞youの候補の辞書を作るために，Greetingsの行からYouの候補を取り出す．\n",
    "    for greetings in mail['greetings']:\n",
    "        if greetings != []:\n",
    "            doc = nlp(greetings[0])\n",
    "            for ent in doc.ents:\n",
    "                if ent.type in ['PERSON']:\n",
    "                    refer_you[message_id] = {\"YOU\":ent.text, \"YOUR\":ent.text + '\\'s'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# その他の代名詞I, MY, ME, WE, OUR, USに関する辞書\n",
    "refer_pronoun = {}\n",
    "for values in mail_df.values:\n",
    "    message_id = values[1]\n",
    "    sender = values[2]\n",
    "    start = re.search(r'(\\(.+\\))',sender).start()\n",
    "    end = re.search(r'(\\(.+\\))',sender).end()\n",
    "    sender = sender[start+1:end-1]\n",
    "    refer_pronoun[message_id]={'I':sender, 'MY':sender + '\\'s', 'ME':sender, 'WE':sender, 'OUR':sender + '\\'s', 'US':sender}\n",
    "\n",
    "# 2つの辞書を結合する\n",
    "for key, value in refer_you.items():\n",
    "    if key in refer_pronoun:\n",
    "        d = refer_pronoun[key]\n",
    "        d.update(value)\n",
    "        refer_pronoun[key] = d\n",
    "    else:\n",
    "        refer_pronoun[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文章からトリプルを抽出する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence_list = []\n",
    "for mail in bodies:  #1通ずつ取り出す\n",
    "    text = ''\n",
    "    # 1行{行番号:文}ずつ取り出し，複数文が含まれた1つの文の連なりにする\n",
    "    for sentence in mail['sentence']: \n",
    "        text = text + list(sentence.values())[0] + ' '\n",
    "    # 文章を文に分解する\n",
    "    if text != '':\n",
    "        doc = nlp(text)\n",
    "        for sentence in doc.sentences:\n",
    "            sentence_list.append((mail['message_id'], sentence.text))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total time:0.6039260665575663 minutes\n"
    }
   ],
   "source": [
    "total_start = time.time()\n",
    "# MinIEにかける\n",
    "import requests\n",
    "import json\n",
    "\n",
    "extractions_list = []\n",
    "for sentence in sentence_list:\n",
    "    message_id = sentence[0]\n",
    "    sentence = sentence[1]\n",
    "    try:\n",
    "        response = requests.post('http://localhost:8080/minie/query', data=sentence)\n",
    "        result = response.json()\n",
    "        if result['facts'] == []:\n",
    "            pass\n",
    "        else:\n",
    "            for triple in result['facts']:\n",
    "                extractions_list.append([message_id, sentence, triple['subject'], triple['predicate'], triple['object']])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# dataframeにする\n",
    "triple_df = pd.DataFrame(extractions_list, columns = ['message_id','sentence', 'subject', 'predicate', 'object'])\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(\"Total time:{} minutes\".format(total_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トリプルを代名詞の辞書と照合し，代名詞が含まれていれば対応するエンティティで置き換える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Senderに置き換えるための辞書を使って実際に置き換える\n",
    "triples = []\n",
    "for row in triple_df.values:\n",
    "    message_id = row[0]\n",
    "    sentence = row[1]\n",
    "    sbj = row[2].split()\n",
    "    pred = row[3].split()\n",
    "    obj = row[4].split()\n",
    "    pronouns = refer_pronoun[message_id]\n",
    "    # subjectの置き換え\n",
    "    for i, word in enumerate(sbj):\n",
    "        entity = pronouns.get(word.upper())\n",
    "        if entity is None:\n",
    "            continue\n",
    "        else:\n",
    "            sbj[i] = entity\n",
    "    sbj = ' '.join(sbj)\n",
    "    # predicateの置き換え\n",
    "    for i, word in enumerate(pred):\n",
    "        entity = pronouns.get(word.upper())\n",
    "        if entity is None:\n",
    "            continue\n",
    "        else:\n",
    "            pred[i] = entity\n",
    "    pred = ' '.join(pred)\n",
    "    # objectの置き換え\n",
    "    for i, word in enumerate(obj):\n",
    "        entity = pronouns.get(word.upper())\n",
    "        if entity is None:\n",
    "            continue\n",
    "        else:\n",
    "            obj[i] = entity\n",
    "    obj = ' '.join(obj)\n",
    "    # [new_arg1, new_rel, new_arg2s]を1行として追加\n",
    "    triples.append((message_id, sentence, sbj, pred, obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トリプルとEntityの辞書を照合し，辞書に登録されているEntityで置き換える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_triples = []\n",
    "mentions = list(integrated_entity_dict.keys())\n",
    "for row in triples:\n",
    "    message_id = row[0]\n",
    "    sentence = row[1]\n",
    "    sbj = row[2]\n",
    "    pred = row[3]\n",
    "    obj = row[4]\n",
    "\n",
    "    # subjectあるいはobjectが辞書に登録されているメンションと一致する場合\n",
    "    if sbj in mentions:\n",
    "        sbj = list(integrated_entity_dict[sbj].keys()) #メンションに対応するEntityで置き換える\n",
    "        sbj = sbj[0]\n",
    "        canonical_triples.append((message_id, sentence, sbj, pred, obj))\n",
    "    elif obj in mentions:\n",
    "        obj = list(integrated_entity_dict[obj].keys()) #メンションに対応するEntityで置き換える\n",
    "        obj = obj[0]\n",
    "        canonical_triples.append((message_id, sentence, sbj, pred, obj))\n",
    "    else:\n",
    "        for mention in mentions:\n",
    "            entity = list(integrated_entity_dict[mention].keys())\n",
    "            entity = entity[0]\n",
    "            if re.search(re.escape(mention), sbj):\n",
    "                canonical_triples.append((message_id, sentence, sbj, pred, obj))\n",
    "                canonical_triples.append((message_id, sentence, entity, 'seeAlso', sbj))\n",
    "            elif re.search(re.escape(mention), obj):\n",
    "                canonical_triples.append((message_id, sentence, sbj, pred, obj))\n",
    "                canonical_triples.append((message_id, sentence, entity, 'seeAlso', obj))\n",
    "\n",
    "canonical_triple_df = pd.DataFrame(canonical_triples, columns=['message_id', 'sentence', 'sbject', 'predicate', 'object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDBにtripleテーブルを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db import connect\n",
    "engine = connect()\n",
    "\n",
    "canonical_triple_df = canonical_triple_df.drop_duplicates()\n",
    "canonical_triple_df.to_sql(name='wiki_research_l_triple',con=engine,if_exists='replace',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}