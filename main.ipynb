{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, email\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import tokenize\n",
    "import pprint, re, time\n",
    "\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loadFile import getFileList, getDirList, fileToDataFrame\n",
    "\n",
    "# ディレクトリ 内のメールファイルを読み込む\n",
    "directory_path = \"wiki-research-l/2020-July\"\n",
    "file_list = getFileList(directory_path)\n",
    "file_list.sort()\n",
    "\n",
    "# テキストファイルをデータフレームに格納する\n",
    "mail_df, thread_df = fileToDataFrame(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies = dict(Bodies=[])\n",
    "for idx, body in mail_df.loc[:,'Body'].items():\n",
    "    origin = []\n",
    "    greetings = dict(Greetings=[])\n",
    "    sentences = dict(Sentence=[])\n",
    "    captions = dict(Caption=[])\n",
    "    bulletlist = dict(Bulletlist=[])\n",
    "    ending = dict(Ending=[])\n",
    "    quotation = dict(Quotation=[])\n",
    "    footer = dict(Footer=[])\n",
    "    misc = dict(Misc=[])\n",
    "\n",
    "    lines = body.splitlines()\n",
    "    for num, line in enumerate(lines):\n",
    "        if re.match(r'\\[G\\]',line) is not None:\n",
    "            greetings['Greetings'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[S\\]',line) is not None:\n",
    "            sentences['Sentence'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[C\\]',line) is not None:\n",
    "            captions['Caption'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[B\\]',line) is not None:\n",
    "            bulletlist['Bulletlist'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[E\\]',line) is not None:\n",
    "            ending['Ending'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[Q\\]',line) is not None:\n",
    "            quotation['Quotation'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[F\\]',line) is not None:\n",
    "            footer['Footer'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[M\\]',line) is not None:\n",
    "            misc['Misc'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        else: #空白行に対応する\n",
    "            continue\n",
    "    originbody = ' '.join(origin)\n",
    "    bodies['Bodies'].append({'idx':idx, 'countrows':len(lines), 'body':originbody, **greetings, **sentences, **captions, **bulletlist, **ending, **quotation, **footer, **misc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#　bodiesのsentenceのvalueから複数文のテキストを作る\n",
    "s_list = []\n",
    "for i in bodies['Bodies']:  #1通ずつ取り出す\n",
    "    text = ''\n",
    "    for j in i['Sentence']: #1行{行番号:文}ずつ取り出し，複数文が含まれた1つの文字列にする\n",
    "        text = text + list(j.values())[0] + ' '\n",
    "    s_list.append(text) #1通ごとの自然文のテキストをリストに格納する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# センテンスのdataframeを作る\n",
    "sentence_list = []\n",
    "# 文章全体に対する前処理\n",
    "for i, content in enumerate(s_list):\n",
    "    sentences = tokenize.sent_tokenize(content)\n",
    "    # 文に対する前処理\n",
    "    for j, sentence in enumerate(sentences):\n",
    "        sentence = re.sub(r'\\s{2,}',' ',sentence)\n",
    "        sentence_list.append([mail_df['Message-ID'][i],sentence])\n",
    "sentence_df = pd.DataFrame(sentence_list,columns=['Message-ID','sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df.head()\n",
    "sentence_df.to_csv('wiki-research-l/output/sentence_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# OpenIEにかける\n",
    "from pyopenie import OpenIE5\n",
    "extractor = OpenIE5('http://localhost:8000')\n",
    "\n",
    "extractions_list = []\n",
    "for i,sentence in sentence_df['sentence'].items():\n",
    "    try:\n",
    "        extractions = extractor.extract(sentence)\n",
    "        extractions_list.append(extractions)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                             message_id  \\\n0  <CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com>   \n1  <CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com>   \n2  <CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com>   \n3  <CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com>   \n4  <CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com>   \n\n                                                                                                          sentence  \\\n0  It's a great suggestion to include a list of ORES based tools for people who are not familiar with ORES itself.   \n1  It's a great suggestion to include a list of ORES based tools for people who are not familiar with ORES itself.   \n2  It's a great suggestion to include a list of ORES based tools for people who are not familiar with ORES itself.   \n3                                    For our research, we would love to interview you and test our visualizations.   \n4                                    For our research, we would love to interview you and test our visualizations.   \n\n                 arg1                 rel  \\\n0              people             are not   \n1  a great suggestion          to include   \n2                  It                  's   \n3                  we  would love to test   \n4                  we          would love   \n\n                                                                 arg2s  \\\n0                                                   familiar with ORES   \n1                                a list of ORES based tools for people   \n2  a great suggestion to include a list of ORES based tools for people   \n3                                                   our visualizations   \n4                          to test our visualizations For our research   \n\n   confidence  \n0    0.895843  \n1    0.925061  \n2    0.678369  \n3    0.256095  \n4    0.380779  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>message_id</th>\n      <th>sentence</th>\n      <th>arg1</th>\n      <th>rel</th>\n      <th>arg2s</th>\n      <th>confidence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com&gt;</td>\n      <td>It's a great suggestion to include a list of ORES based tools for people who are not familiar with ORES itself.</td>\n      <td>people</td>\n      <td>are not</td>\n      <td>familiar with ORES</td>\n      <td>0.895843</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com&gt;</td>\n      <td>It's a great suggestion to include a list of ORES based tools for people who are not familiar with ORES itself.</td>\n      <td>a great suggestion</td>\n      <td>to include</td>\n      <td>a list of ORES based tools for people</td>\n      <td>0.925061</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com&gt;</td>\n      <td>It's a great suggestion to include a list of ORES based tools for people who are not familiar with ORES itself.</td>\n      <td>It</td>\n      <td>'s</td>\n      <td>a great suggestion to include a list of ORES based tools for people</td>\n      <td>0.678369</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com&gt;</td>\n      <td>For our research, we would love to interview you and test our visualizations.</td>\n      <td>we</td>\n      <td>would love to test</td>\n      <td>our visualizations</td>\n      <td>0.256095</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com&gt;</td>\n      <td>For our research, we would love to interview you and test our visualizations.</td>\n      <td>we</td>\n      <td>would love</td>\n      <td>to test our visualizations For our research</td>\n      <td>0.380779</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 143
    }
   ],
   "source": [
    "\n",
    "# OIEの抽出結果を整形する\n",
    "rows = []\n",
    "for i, extractions in enumerate(extractions_list):\n",
    "    if extractions == []:\n",
    "        pass\n",
    "    else:\n",
    "        for extraction in extractions:\n",
    "            message_id = sentence_df['Message-ID'][i]\n",
    "            sentence = extraction['sentence']\n",
    "            confidence = extraction['confidence']\n",
    "            arg1 = extraction['extraction']['arg1']['text']\n",
    "            rel = extraction['extraction']['rel']['text']\n",
    "            arg2s_list = []\n",
    "            for arg2 in extraction['extraction']['arg2s']:\n",
    "                arg2s_list.append(arg2['text'])\n",
    "            arg2s = ' '.join(map(str, arg2s_list))\n",
    "            row = [message_id, sentence, arg1, rel, arg2s, confidence]\n",
    "            rows.append(row)\n",
    "\n",
    "# 整形結果をdataframeにする\n",
    "kb_df = pd.DataFrame(rows, columns = ['message_id','sentence', 'arg1', 'rel', 'arg2s', 'confidence'])\n",
    "kb_df.head()\n",
    "# dataframeをcsv出力する\n",
    "#kb_df.to_csv('wiki-research-l/output/triple_from_text_part.csv')\n",
    "\n",
    "# dataframeをRDBのテーブルにする\n",
    "#from db import connect\n",
    "#engine = connect()\n",
    "#kb_df.to_sql(name='kb_wiki_research_l_text',con=engine,if_exists='replace',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from entityLinking import tagme, confidentAnnotations, mediaWiki\n",
    "\n",
    "entity_list = []\n",
    "for i, sentence in sentence_df['sentence'].items():\n",
    "    json_res = tagme(sentence)\n",
    "    for candidate in json_res['annotations']:\n",
    "        if candidate['rho'] >= 0.3:\n",
    "            entity_list.append([i, candidate['spot'],candidate['rho'],candidate['id'],candidate['title']])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# 抽出したentityをdataframeに格納する\n",
    "rows = []\n",
    "for row in entity_list:\n",
    "    rows.append([sentence_df['Message-ID'][row[0]], row[1], row[2], row[3], row[4]])\n",
    "entity_df = pd.DataFrame(rows,columns=['Message-ID','spot','rho','id','title'])\n",
    "entity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entity_df = pd.read_csv('wiki-research-l/output/entity_df.csv', index_col=0)\n",
    "entity_df[entity_df['rho'] < 0.3][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tripleのI, MY, ME, WE, OUR, USの代名詞をSenderに置き換えるための辞書\n",
    "refer_pronoun = {}\n",
    "for row in kb_df.values:\n",
    "    message_id = row[0]\n",
    "    sender = mail_df[mail_df['Message-ID']==message_id]['From'].values[0]\n",
    "    start = re.search(r'(\\(.+\\))',sender).start()\n",
    "    end = re.search(r'(\\(.+\\))',sender).end()\n",
    "    sender = sender[start+1:end-1]\n",
    "    refer_pronoun[message_id]={'I':sender, 'MY':sender + '\\'s', 'ME':sender, 'WE':sender, 'OUR':sender + '\\'s', 'US':sender}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Senderに置き換えるための辞書を使って実際に置き換える\n",
    "replaced_rows = []\n",
    "for row in kb_df.values:\n",
    "    message_id = row[0]\n",
    "    arg1 = row[2].split()\n",
    "    rel = row[3].split()\n",
    "    arg2s = row[4].split()\n",
    "    replaced = []\n",
    "    for i, word in enumerate(arg1):\n",
    "        sender = refer_pronoun[message_id].get(word.upper())\n",
    "        if sender is None:\n",
    "            continue\n",
    "        else:\n",
    "            arg1[i] = sender\n",
    "    new_arg1 = ' '.join(arg1)\n",
    "    replaced.append(new_arg1)\n",
    "    for i, word in enumerate(rel):\n",
    "        sender = refer_pronoun[message_id].get(word.upper())\n",
    "        if sender is None:\n",
    "            continue\n",
    "        else:\n",
    "            rel[i] = sender\n",
    "    new_rel = ' '.join(rel)\n",
    "    replaced.append(new_rel)\n",
    "    for i, word in enumerate(arg2s):\n",
    "        sender = refer_pronoun[message_id].get(word.upper())\n",
    "        if sender is None:\n",
    "            continue\n",
    "        else:\n",
    "            arg2s[i] = sender\n",
    "    new_arg2s = ' '.join(arg2s)\n",
    "    replaced.append(new_arg2s)\n",
    "    replaced_rows.append(replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_triples = pd.DataFrame(replaced_rows,columns=['new_arg1','new_rel','new_arg2s'])\n",
    "kb_df = pd.concat([kb_df, replaced_triples],axis=1)\n",
    "kb_df.to_csv('/Users/taroaso/myprojects/OpenIE/wiki-research-l/output/replaced_triple_from_text_part.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extraction import entityExtraction\n",
    "\n",
    "ner_list = []\n",
    "for i, sentence in sentence_df['sentence'].items():\n",
    "    entities = entityExtraction(sentence)\n",
    "    ner_list.append(entities)\n",
    "len(ner_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}