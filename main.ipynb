{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys, email, datetime, pprint, re, time, html\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.metrics import *\n",
    "\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### 自然言語処理\n",
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.pipeline import Sentencizer\n",
    "sentencizer = Sentencizer()\n",
    "nlp.add_pipe(sentencizer)\n",
    "\n",
    "#import stanza\n",
    "#stanza.download('en')\n",
    "#nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メールファイルからmail_dfを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loadFile import getFileList\n",
    "\n",
    "# ディレクトリ 内のメールファイルを読み込む\n",
    "directory_path = \"/Users/taroaso/myprojects/OpenIE/trec/2005/each_dataset/3\"\n",
    "file_list = getFileList(directory_path)\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "### mail_df \n",
    "\n",
    "mail_cols = ['docno','received','isoreceived','sent','isosent','name','email','subject','id','charset','inreplyto','expires','to','cc','body']\n",
    "mail_df = pd.DataFrame(index=[], columns=mail_cols)\n",
    "\n",
    "for file in file_list:\n",
    "    with open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        mail = f.readlines()\n",
    "    \n",
    "        record={}\n",
    "        body = []\n",
    "        for row in mail:\n",
    "            if row.startswith('docno='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['docno'] = match.group().strip('\"')\n",
    "            elif row.startswith('received='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['received'] = match.group().strip('\"')\n",
    "            elif row.startswith('isoreceived='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    rt = match.group().strip('\"')\n",
    "                    record['isoreceived'] = datetime.datetime(int(rt[0:4]),int(rt[4:6]),int(rt[6:8]),int(rt[8:10]),int(rt[10:12]),int(rt[12:14])) #yyyy,mm,dd,hh,mm,ss\n",
    "            elif row.startswith('sent='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['sent'] = match.group().strip('\"')\n",
    "            elif row.startswith('isosent='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    st = match.group().strip('\"')\n",
    "                    record['isosent'] = datetime.datetime(int(st[0:4]),int(st[4:6]),int(st[6:8]),int(st[8:10]),int(st[10:12]),int(st[12:14])) #yyyy,mm,dd,hh,mm,ss\n",
    "            elif row.startswith('name='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['name'] = match.group().strip('\"')\n",
    "            elif row.startswith('email='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['email'] = match.group().strip('\"')\n",
    "            elif row.startswith('subject='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    match = match.group().strip('\"')\n",
    "                    match = html.unescape(match) # subjectに含まれるHTML特殊文字をユニコード文字に変換する\n",
    "                    record['subject'] = match\n",
    "            elif row.startswith('id='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['id'] = match.group().strip('\"')\n",
    "            elif row.startswith('charset='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['charset'] = match.group().strip('\"')\n",
    "            elif row.startswith('inreplyto='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    match = match.group().strip('\"')\n",
    "                    match = html.unescape(match) # inreplytoのHTML特殊文字をユニコード文字に変換する\n",
    "                    record['inreplyto'] = match\n",
    "            elif row.startswith('expires='):\n",
    "                match = re.search(r'\".+\"',row)\n",
    "                if match != None:\n",
    "                    record['expires'] = match.group().strip('\"')\n",
    "            elif row.startswith('To:'):\n",
    "                match = row[3:-1]\n",
    "                record.setdefault('to',match)\n",
    "            elif row.startswith('Cc:'):\n",
    "                match = row[3:-1]\n",
    "                record.setdefault('cc',match)\n",
    "            else:\n",
    "                body.append(row)\n",
    "        record['body'] = ''.join(body)\n",
    "    \n",
    "    mail_df = mail_df.append(record, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                docno                  received         isoreceived  \\\n0  lists-000-13503992  Fri Jul 14 12:48:10 2000 2000-07-14 16:48:10   \n1   lists-001-9920622  Sat Jul 14 07:24:02 2001 2001-07-14 11:24:02   \n2  lists-002-10134409  Tue Apr 29 10:23:48 2003 2003-04-29 14:23:48   \n3  lists-002-10233238  Sat May 17 03:24:47 2003 2003-05-17 07:24:47   \n4  lists-002-15997816  Sat Nov 22 17:30:35 2003 2003-11-22 22:30:35   \n\n                                    sent             isosent  \\\n0  Fri, 14 Jul 2000 08:19:41 -0400 (EDT) 2000-07-14 12:19:41   \n1        Sat, 14 Jul 2001 13:12:58 +0200 2001-07-14 11:12:58   \n2        Tue, 29 Apr 2003 22:23:41 +0800 2003-04-29 14:23:41   \n3  Wed, 14 May 2003 09:32:46 -0400 (EDT) 2003-05-14 13:32:46   \n4        Wed, 19 Nov 2003 18:42:25 -0800 2003-11-20 02:42:25   \n\n                  name                     email  \\\n0       Georgopoulos N  ngeorgopoulos@city.ac.uk   \n1  Karl Ove Hufthammer        huftis@bigfoot.com   \n2            lock mode        klcc70@hotmail.com   \n3       David Van Cott            dtvc17@msn.com   \n4               Azigan            azigan@fix.net   \n\n                                                                         subject  \\\n0                                                                     Bug report   \n1                                                     Re: entities in title tags   \n2  Stock markets rise on positive earnings, consumer spending ConsumerReportsorg   \n3                                                                    Tag problem   \n4                          www.worldwidewoodworking would like to exchange links   \n\n                                        id     charset  \\\n0             E13D4Rs-0004TB-00@mailswitch    US-ASCII   \n1    003b01c10c55$f0f62060$f3468ed5@huftis  iso-8859-1   \n2  Law10-F458juXym3SjK00016b82@hotmail.com         NaN   \n3  BAY4-F144h4Jzgi1Z8200004355@hotmail.com         NaN   \n4    0HOM00LVHQ68TB@a34-mta02.direcway.com         NaN   \n\n                                 inreplyto expires  \\\n0                                      NaN      -1   \n1  000201c10c12$b894f900$4222e540@IanEvans      -1   \n2                                      NaN      -1   \n3                                      NaN      -1   \n4                                      NaN      -1   \n\n                                                           to          cc  \\\n0                                            html-tidy@w3.org         NaN   \n1  &quot;Ian M. Evans&quot;&lt;ianevans@digitalhit.com&gt;,&lt;html-tidy@w3.org&gt;         NaN   \n2                                            html-tidy@w3.org         NaN   \n3                                            html-tidy@w3.org  dsr@w3.org   \n4                                 Html Tidy&lt;html-tidy@w3.org&gt;         NaN   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      body  \n0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\n\\n\\n\\n\\ntext/enriched attachment: stored\\n\\n\\n\\n\\n  \n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n\\n\\n----- Original Message ----- \\nFrom: &quot;Ian M. Evans&quot; &lt;ianevans@digitalhit.com&gt;\\nSent: Saturday, July 14, 2001 5:11 AM\\nSubject: entities in title tags\\n\\n\\n&gt; Just wondering if there&#39;s anything wrong with entities in title tags.\\n\\nNo.\\n\\n&gt; I&#39;ve noticed that the numeric entity for the apostrophe ends up showing up\\n&gt; in Google search results as:\\n\\nThis is a bug in Google.\\n\\n-- \\nKarl Ove Hufthammer\\n\\n\\n\\n  \n2  \\n\\n\\n\\nUnbiased product Ratings from the experts at Product Ratings, product \\nreviews, buying guides, product safety recalls and consumer information from \\nthe experts at Consumer Union/Consumer Reports Magazine. Information, \\nratings, and advice on products, services, and decisions. new car reviews, \\nused car Consumer Reports Cars and Trucks: Stay informed with our latest new \\ncar reviews, used car reviews, auto ratings, car safety reports and auto \\nprice service. Provides car reviews, automobile safety information, car \\nbuying guidance.\\n\\n\\n\\nhttp://www.geocities.com/webrobotic2/ConsumerReports.htm\\n\\n\\nconsumer report\\n12965  consumer report magazine\\n11859  consumer report car\\n9172  consumer report online\\n7784  consumer report auto\\n4280  consumer report vacuum cleaners\\n3970  consumer report digital camera\\n3764  consumer credit report\\n3633  consumer report automobile\\n2817  free consumer report\\n2477  consumer report mattress\\n2221  consumer report on health\\n1978...  \n3                                                                                            \\n\\n\\n\\n\\n\\n\\nI&#39;m using html tidy at work to make the website more accessible and I&#39;m \\nhaving a little problem.  Everything seems to work great execept for one \\nthing. Tidy seems to be removing the &lt;dl&gt; tags.  This is causing the \\nalignment of the webpages to be off.  I searched through google newsgroups \\nand the www.w3.org site and I couldnt find any questions or solutions \\nregarding this.  There is my configs...\\n--clean yes\\n--output-xml no\\n--enclose-text yes\\n--enclose-block-text yes\\n--indent auto\\n--alt-text GraphicImage\\n\\nOther then that problem I think this program is great.  I have to make sure \\nover 1000 html files have alt attributes inside the img tag.  This make file \\nmuch similar.  Thanks\\nDavid\\n\\n_________________________________________________________________\\nHelp STOP SPAM with the new MSN 8 and get 2 months FREE*  \\nhttp://join.msn.com/?page=features/junkmail\\n\\n\\n\\n  \n4                                                                                                                                                                                                                                                 \\n\\n\\n\\nWe both know the importance on having multiple links on the internet. \\nThe more links we have \\nthe easier it is for robots to spider and the result is that we get \\nmore hits to our webpages.\\nPlease link to my page and send me your info so that I can add a link \\nto your site.\\n\\nZigans Woodcrafts your online plans store\\n\\nhttp://www.worldwidewoodworking.com\\n My Logo is at:\\nhttp://www.worldwidewoodworking.com/zigan-logo-small.gif\\n\\nOr copy and paste this banner code:\\n&lt;a href=&quot;http://www.worldwidewoodworking.com&quot;&gt;&lt;IMG\\nSRC=&quot;http://www.worldwidewoodworking.com/zigan-logo-small.gif&quot;\\nALT=&quot;Zigan&#39;s Woodcrafts. We are adding new woodworking plans weekly for all\\nlevels of\\nwoodworking&quot;&gt;&lt;/A&gt;\\n\\n\\n\\nThank You for your time\\nAndy Zigan\\n\\n\\n\\n  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>docno</th>\n      <th>received</th>\n      <th>isoreceived</th>\n      <th>sent</th>\n      <th>isosent</th>\n      <th>name</th>\n      <th>email</th>\n      <th>subject</th>\n      <th>id</th>\n      <th>charset</th>\n      <th>inreplyto</th>\n      <th>expires</th>\n      <th>to</th>\n      <th>cc</th>\n      <th>body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>lists-000-13503992</td>\n      <td>Fri Jul 14 12:48:10 2000</td>\n      <td>2000-07-14 16:48:10</td>\n      <td>Fri, 14 Jul 2000 08:19:41 -0400 (EDT)</td>\n      <td>2000-07-14 12:19:41</td>\n      <td>Georgopoulos N</td>\n      <td>ngeorgopoulos@city.ac.uk</td>\n      <td>Bug report</td>\n      <td>E13D4Rs-0004TB-00@mailswitch</td>\n      <td>US-ASCII</td>\n      <td>NaN</td>\n      <td>-1</td>\n      <td>html-tidy@w3.org</td>\n      <td>NaN</td>\n      <td>\\n\\n\\n\\n\\ntext/enriched attachment: stored\\n\\n\\n\\n\\n</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>lists-001-9920622</td>\n      <td>Sat Jul 14 07:24:02 2001</td>\n      <td>2001-07-14 11:24:02</td>\n      <td>Sat, 14 Jul 2001 13:12:58 +0200</td>\n      <td>2001-07-14 11:12:58</td>\n      <td>Karl Ove Hufthammer</td>\n      <td>huftis@bigfoot.com</td>\n      <td>Re: entities in title tags</td>\n      <td>003b01c10c55$f0f62060$f3468ed5@huftis</td>\n      <td>iso-8859-1</td>\n      <td>000201c10c12$b894f900$4222e540@IanEvans</td>\n      <td>-1</td>\n      <td>\"Ian M. Evans\"&lt;ianevans@digitalhit.com&gt;,&lt;html-tidy@w3.org&gt;</td>\n      <td>NaN</td>\n      <td>\\n\\n\\n----- Original Message ----- \\nFrom: \"Ian M. Evans\" &lt;ianevans@digitalhit.com&gt;\\nSent: Saturday, July 14, 2001 5:11 AM\\nSubject: entities in title tags\\n\\n\\n&gt; Just wondering if there's anything wrong with entities in title tags.\\n\\nNo.\\n\\n&gt; I've noticed that the numeric entity for the apostrophe ends up showing up\\n&gt; in Google search results as:\\n\\nThis is a bug in Google.\\n\\n-- \\nKarl Ove Hufthammer\\n\\n\\n\\n</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>lists-002-10134409</td>\n      <td>Tue Apr 29 10:23:48 2003</td>\n      <td>2003-04-29 14:23:48</td>\n      <td>Tue, 29 Apr 2003 22:23:41 +0800</td>\n      <td>2003-04-29 14:23:41</td>\n      <td>lock mode</td>\n      <td>klcc70@hotmail.com</td>\n      <td>Stock markets rise on positive earnings, consumer spending ConsumerReportsorg</td>\n      <td>Law10-F458juXym3SjK00016b82@hotmail.com</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1</td>\n      <td>html-tidy@w3.org</td>\n      <td>NaN</td>\n      <td>\\n\\n\\n\\nUnbiased product Ratings from the experts at Product Ratings, product \\nreviews, buying guides, product safety recalls and consumer information from \\nthe experts at Consumer Union/Consumer Reports Magazine. Information, \\nratings, and advice on products, services, and decisions. new car reviews, \\nused car Consumer Reports Cars and Trucks: Stay informed with our latest new \\ncar reviews, used car reviews, auto ratings, car safety reports and auto \\nprice service. Provides car reviews, automobile safety information, car \\nbuying guidance.\\n\\n\\n\\nhttp://www.geocities.com/webrobotic2/ConsumerReports.htm\\n\\n\\nconsumer report\\n12965  consumer report magazine\\n11859  consumer report car\\n9172  consumer report online\\n7784  consumer report auto\\n4280  consumer report vacuum cleaners\\n3970  consumer report digital camera\\n3764  consumer credit report\\n3633  consumer report automobile\\n2817  free consumer report\\n2477  consumer report mattress\\n2221  consumer report on health\\n1978...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>lists-002-10233238</td>\n      <td>Sat May 17 03:24:47 2003</td>\n      <td>2003-05-17 07:24:47</td>\n      <td>Wed, 14 May 2003 09:32:46 -0400 (EDT)</td>\n      <td>2003-05-14 13:32:46</td>\n      <td>David Van Cott</td>\n      <td>dtvc17@msn.com</td>\n      <td>Tag problem</td>\n      <td>BAY4-F144h4Jzgi1Z8200004355@hotmail.com</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1</td>\n      <td>html-tidy@w3.org</td>\n      <td>dsr@w3.org</td>\n      <td>\\n\\n\\n\\n\\n\\n\\nI'm using html tidy at work to make the website more accessible and I'm \\nhaving a little problem.  Everything seems to work great execept for one \\nthing. Tidy seems to be removing the &lt;dl&gt; tags.  This is causing the \\nalignment of the webpages to be off.  I searched through google newsgroups \\nand the www.w3.org site and I couldnt find any questions or solutions \\nregarding this.  There is my configs...\\n--clean yes\\n--output-xml no\\n--enclose-text yes\\n--enclose-block-text yes\\n--indent auto\\n--alt-text GraphicImage\\n\\nOther then that problem I think this program is great.  I have to make sure \\nover 1000 html files have alt attributes inside the img tag.  This make file \\nmuch similar.  Thanks\\nDavid\\n\\n_________________________________________________________________\\nHelp STOP SPAM with the new MSN 8 and get 2 months FREE*  \\nhttp://join.msn.com/?page=features/junkmail\\n\\n\\n\\n</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>lists-002-15997816</td>\n      <td>Sat Nov 22 17:30:35 2003</td>\n      <td>2003-11-22 22:30:35</td>\n      <td>Wed, 19 Nov 2003 18:42:25 -0800</td>\n      <td>2003-11-20 02:42:25</td>\n      <td>Azigan</td>\n      <td>azigan@fix.net</td>\n      <td>www.worldwidewoodworking would like to exchange links</td>\n      <td>0HOM00LVHQ68TB@a34-mta02.direcway.com</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1</td>\n      <td>Html Tidy&lt;html-tidy@w3.org&gt;</td>\n      <td>NaN</td>\n      <td>\\n\\n\\n\\nWe both know the importance on having multiple links on the internet. \\nThe more links we have \\nthe easier it is for robots to spider and the result is that we get \\nmore hits to our webpages.\\nPlease link to my page and send me your info so that I can add a link \\nto your site.\\n\\nZigans Woodcrafts your online plans store\\n\\nhttp://www.worldwidewoodworking.com\\n My Logo is at:\\nhttp://www.worldwidewoodworking.com/zigan-logo-small.gif\\n\\nOr copy and paste this banner code:\\n&lt;a href=\"http://www.worldwidewoodworking.com\"&gt;&lt;IMG\\nSRC=\"http://www.worldwidewoodworking.com/zigan-logo-small.gif\"\\nALT=\"Zigan's Woodcrafts. We are adding new woodworking plans weekly for all\\nlevels of\\nwoodworking\"&gt;&lt;/A&gt;\\n\\n\\n\\nThank You for your time\\nAndy Zigan\\n\\n\\n\\n</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "mail_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# toの正規化処理\n",
    "canonical_step1 = []\n",
    "for value in mail_df.values:\n",
    "    to = value[12]\n",
    "    if type(to) != str: #型チェック（nanである場合がある）\n",
    "        value[12] = None\n",
    "        canonical_step1.append(list(value))\n",
    "    else:\n",
    "        emails = re.findall(r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)',to)\n",
    "        if emails != []:\n",
    "            for email in emails:\n",
    "                value[12] = email\n",
    "                canonical_step1.append(list(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ccの正規化処理\n",
    "canonical_step2 = []\n",
    "for value in canonical_step1:\n",
    "    cc = value[13]\n",
    "    if type(cc) != str: #型チェック（nanである場合がある）\n",
    "        value[13] = None\n",
    "        canonical_step2.append(list(value))\n",
    "    else:\n",
    "        emails = re.findall(r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)',cc)\n",
    "        if emails != []:\n",
    "            for email in emails:\n",
    "                value[13] = email\n",
    "                canonical_step2.append(list(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1048"
     },
     "metadata": {},
     "execution_count": 177
    }
   ],
   "source": [
    "len(canonical_step2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_mail_df = pd.DataFrame(canonical_step2,columns=mail_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{&#39;ngeorgopoulos@city.ac.uk&#39;: &#39;Georgopoulos N&#39;,\n &#39;huftis@bigfoot.com&#39;: &#39;Karl Ove Hufthammer&#39;,\n &#39;klcc70@hotmail.com&#39;: &#39;lock mode&#39;,\n &#39;dtvc17@msn.com&#39;: &#39;David Van Cott&#39;,\n &#39;azigan@fix.net&#39;: &#39;Azigan&#39;,\n &#39;surfing_worldwide@hotmail.com&#39;: &quot;Surfer&#39;s Choice&quot;,\n &#39;hbaughan@rocksolidsite.com&#39;: &#39;Harold Baughan [RockSolidSite.com]&#39;,\n &#39;jehad2004@ummah.org&#39;: &#39;jehad2004@ummah.org&#39;,\n &#39;cadasaqui@hotmail.com&#39;: &#39;Incluimos seu site&#39;,\n &#39;cadastretexto@hotmail.com&#39;: &#39;Proposta&#39;,\n &#39;bJ@light-avenue.com&#39;: &#39;bJ@light-avenue.com&#39;,\n &#39;geoffrey.clemm@rational.com&#39;: &#39;Geoffrey M. Clemm&#39;,\n &#39;yms318f7788uu@yahoo.co.kr&#39;: &#39;????&#39;,\n &#39;fthnzpkebumnqu@fce.vutbr.cz&#39;: &#39;Cecile Barnard&#39;,\n &#39;gclemm@rational.com&#39;: &#39;Clemm, Geoff&#39;,\n &#39;masinter@parc.xerox.com&#39;: &#39;Larry Masinter&#39;,\n &#39;taha.masood@streamingnetworks.com&#39;: &#39;Taha Masood&#39;,\n &#39;koen@win.tue.nl&#39;: &#39;Koen Holtman&#39;,\n &#39;jyunker@bytelevel.com&#39;: &#39;John Yunker&#39;,\n &#39;danbri@w3.org&#39;: &#39;Dan Brickley&#39;,\n &#39;danield@w3.org&#39;: &#39;Daniel Dardailler&#39;,\n &#39;charles@w3.org&#39;: &#39;Charles McCathieNevile&#39;,\n &#39;Libby.Miller@bristol.ac.uk&#39;: &#39;Libby Miller&#39;,\n &#39;coralie@w3.org&#39;: &#39;Coralie Mercier&#39;,\n &#39;Kate.Sharp@bristol.ac.uk&#39;: &#39;Kate Sharp&#39;,\n &#39;M.D.Wilson@rl.ac.uk&#39;: &#39;Wilson, MD (Michael) &#39;,\n &#39;ishida@w3.org&#39;: &#39;Richard Ishida&#39;,\n &#39;tex@i18nguy.com&#39;: &#39;Tex Texin&#39;,\n &#39;aphillips@webmethods.com&#39;: &#39;Addison Phillips [wM]&#39;,\n &#39;plh@w3.org&#39;: &#39;Philippe Le Hegaret&#39;,\n &#39;lorrie@research.att.com&#39;: &#39;Lorrie Cranor&#39;,\n &#39;rigo@w3.org&#39;: &#39;Rigo Wenning&#39;,\n &#39;giles.hogben@jrc.it&#39;: &#39;Giles Hogben&#39;,\n &#39;Michael.Kay@softwareag.com&#39;: &#39;Kay, Michael&#39;,\n &#39;eric@w3.org&#39;: &quot;Eric Prud&#39;hommeaux&quot;,\n &#39;connolly@w3.org&#39;: &#39;by way of &#39;,\n &#39;welty@us.ibm.com&#39;: &#39;Christopher Welty&#39;,\n &#39;bernard.vatant@mondeca.com&#39;: &#39;Bernard Vatant&#39;,\n &#39;ij@w3.org&#39;: &#39;Ian B. Jacobs&#39;,\n &#39;a.tate@ed.ac.uk&#39;: &#39;Austin Tate&#39;,\n &#39;jason.cunliffe@verizon.net&#39;: &#39;Jason Cunliffe&#39;,\n &#39;ChBussler@aol.com&#39;: &#39;ChBussler@aol.com&#39;,\n &#39;michael.mauch@gmx.de&#39;: &#39;Michael Mauch&#39;,\n &#39;lesch@w3.org&#39;: &#39;Susan Lesch&#39;,\n &#39;Jonathan.Boylan@fineos.com&#39;: &#39;Jonathan Boylan (ext. 724)&#39;,\n &#39;henri@w3.org&#39;: &#39;Henri Fallon&#39;,\n &#39;w3c-hkust@ust.hk&#39;: &#39;W3C Office in HK&#39;,\n &#39;malcolm@reviewmedia.com&#39;: &#39;by way of Susan Lesch&#39;,\n &#39;j.chetwynd@btinternet.com&#39;: &#39;Jonathan Chetwynd&#39;,\n &#39;lynn.alford@jcu.edu.au&#39;: &#39;lynn alford&#39;,\n &#39;cdwise@wiserways.com&#39;: &#39;Cheryl D. Wise&#39;,\n &#39;mike@skew.org&#39;: &#39;Mike Brown&#39;,\n &#39;dmeranda@iac.net&#39;: &#39;Deron Meranda&#39;,\n &#39;podtal@yahoo.com&#39;: &#39;Intro Interactive&#39;,\n &#39;josh@band-ayd.com&#39;: &#39;Josh Becigneul&#39;,\n &#39;charles@sidar.org&#39;: &#39;Charles McCathieNevile&#39;,\n &#39;adfunk01@yahoo.com&#39;: &#39;email packet&#39;,\n &#39;adfunk166@yahoo.com&#39;: &#39;email packet&#39;,\n &#39;adfunk142@yahoo.com&#39;: &#39;email packet&#39;,\n &#39;ibcnet@bcpl.net&#39;: &#39;ibcnet&#39;,\n &#39;nichandros@juno.com&#39;: &#39;Harry Nichandros&#39;,\n &#39;acgi2000@hotmail.com&#39;: &#39;clip_image002.gif&#39;,\n &#39;pcp@post.harvard.edu&#39;: &#39;Paul Pedersen&#39;,\n &#39;janet@w3.org&#39;: &#39;Janet Daly&#39;,\n &#39;jeff@wdc.wa.gov.au&#39;: &#39;Jeff Major&#39;,\n &#39;ianf@yesl.co.uk&#39;: &#39;Ian Fleeton&#39;,\n &#39;info@afba.de&#39;: &#39;Bernd Marscheider&#39;,\n &#39;info@msm-consulting.de&#39;: &#39;MSM - Consulting&#39;,\n &#39;velocipraetorian3@yahoo.it&#39;: &#39;martingorevich martinagorevich&#39;,\n &#39;velocipraetorian@yahoo.no&#39;: &#39;martingorevich martinagorevich&#39;,\n &#39;blewis@atsincorp.com&#39;: &#39;Ben Lewis&#39;,\n &#39;office@nailexpress.de&#39;: &#39;office@nailexpress.de&#39;,\n &#39;islamconvert@ummah.org&#39;: &#39;islamconvert@ummah.org&#39;,\n &#39;publisher@3m7.net&#39;: &#39;Hans Schnauber&#39;,\n &#39;nocity@onenet.com.ar&#39;: &#39;Cityeconomika.com&#39;,\n &#39;deryua@citiz.net&#39;: &#39;deryua@citiz.net&#39;,\n &#39;Internet-Drafts@CNRI.Reston.VA.US&#39;: &#39;Internet-Drafts@CNRI.Reston.VA.US&#39;,\n &#39;godfreyrust@dds.netkonect.co.uk&#39;: &#39;Godfrey Rust&#39;,\n &#39;fielding@gbiv.com&#39;: &#39;Roy T. Fielding&#39;,\n &#39;distobj@acm.org&#39;: &#39;Mark Baker&#39;,\n &#39;dan@tobias.name&#39;: &#39;Daniel R. Tobias&#39;,\n &#39;gjw@wnetc.com&#39;: &#39;Gregory J. Woodhouse&#39;,\n &#39;geuer-pollmann@nue.et-inf.uni-siegen.de&#39;: &#39;Christian Geuer-Pollmann&#39;,\n &#39;dave.beckett@bristol.ac.uk&#39;: &#39;Dave Beckett&#39;,\n &#39;fmanola@mitre.org&#39;: &#39;Frank Manola&#39;,\n &#39;phayes@ai.uwf.edu&#39;: &#39;pat hayes&#39;,\n &#39;patrick.stickler@nokia.com&#39;: &#39;Patrick Stickler&#39;,\n &#39;jos.deroo@agfa.com&#39;: &#39;Jos De_Roo&#39;,\n &#39;papresco@calum.csclub.uwaterloo.ca&#39;: &#39;Paul Prescod&#39;,\n &#39;duerst@w3.org&#39;: &#39;Martin Duerst&#39;,\n &#39;nori@washitake.com&#39;: &#39;Norihisa Washitake&#39;,\n &#39;zabuta@rzva.com.ua&#39;: &#39;?????? ?????&#39;,\n &#39;w3@hotbox.ru&#39;: &#39;Alexander Savenkov&#39;,\n &#39;jan.richards@utoronto.ca&#39;: &#39;Jan Richards&#39;,\n &#39;karl.hebenstreit@gsa.gov&#39;: &#39;karl.hebenstreit@gsa.gov&#39;,\n &#39;kathleen.anderson@po.state.ct.us&#39;: &#39;Kathleen Anderson&#39;,\n &#39;olleo@sics.se&#39;: &#39;Olle Olsson&#39;,\n &#39;karlhjr@comcast.net&#39;: &#39;Karl Hebenstreit, Jr.&#39;,\n &#39;jbrewer@w3.org&#39;: &#39;Judy Brewer&#39;,\n &#39;hbingham@acm.org&#39;: &#39;Harvey Bingham&#39;,\n &#39;kasday@acm.org&#39;: &#39;Leonard R. Kasday&#39;,\n &#39;wendy@w3.org&#39;: &#39;Wendy A Chisholm&#39;,\n &#39;jasonw@ariel.ucs.unimelb.EDU.AU&#39;: &#39;Jason White&#39;,\n &#39;colin@the-net-effect.com&#39;: &#39;Colin F Reynolds&#39;,\n &#39;flavell@a5.ph.gla.ac.uk&#39;: &#39;Alan J. Flavell&#39;,\n &#39;cynthia.waddell@psinetcs.com&#39;: &#39;Cynthia Waddell&#39;,\n &#39;sean@mysterylights.com&#39;: &#39;Sean B. Palmer&#39;,\n &#39;cyns@opendesign.com&#39;: &#39;Cynthia Shelly&#39;,\n &#39;kynn-edapta@idyllmtn.com&#39;: &#39;Kynn Bartlett&#39;,\n &#39;Bruce_Bailey@ed.gov&#39;: &#39;Bailey, Bruce&#39;,\n &#39;seeman@netvision.net.il&#39;: &#39;Lisa Seeman&#39;,\n &#39;asgilman@iamdigex.net&#39;: &#39;Al Gilman&#39;,\n &#39;phoenixl@sonic.net&#39;: &#39;phoenixl&#39;,\n &#39;jens.meiert@erde3.com&#39;: &#39;Jens Meiert&#39;,\n &#39;tcroucher@netalleynetworks.com&#39;: &#39;Tom Croucher&#39;,\n &#39;david@dorward.me.uk&#39;: &#39;David Dorward&#39;,\n &#39;leeroberts@roserockdesign.com&#39;: &#39;Lee Roberts&#39;,\n &#39;dburnett@sesa.org&#39;: &#39;Doyle Burnett&#39;,\n &#39;gv@trace.wisc.edu&#39;: &#39;Gregg Vanderheiden&#39;,\n &#39;kelly@kellford.com&#39;: &#39;Kelly Ford&#39;,\n &#39;rscano@iwa-italy.org&#39;: &#39;Roberto Scano - IWA/HWG&#39;,\n &#39;michaelc@watchfire.com&#39;: &#39;Michael Cooper&#39;,\n &#39;inekemaa@xs4all.nl&#39;: &#39;Ineke van der Maat&#39;,\n &#39;E.Velleman@bartimeus.nl&#39;: &#39;Eric Velleman&#39;,\n &#39;charlesn@sunrise.srl.rmit.edu.au&#39;: &#39;Charles McCathieNevile&#39;,\n &#39;rcn@fenix2.dol-esa.gov&#39;: &#39;Robert C. Neff&#39;,\n &#39;sitekre8@pacbell.net&#39;: &#39;Judy Schnitzer&#39;,\n &#39;unagi69@concentric.net&#39;: &#39;Gregory J. Rosmaita&#39;,\n &#39;bbailey@clark.net&#39;: &#39;Bruce Bailey&#39;,\n &#39;poehlman@clark.net&#39;: &#39;David Poehlman&#39;,\n &#39;access@javawoman.com&#39;: &#39;Marjolein Katsma&#39;,\n &#39;jay@peepo.com&#39;: &#39;Jonathan Chetwynd&#39;,\n &#39;rbrown@blackboard.com&#39;: &#39;Reidy Brown&#39;,\n &#39;robneff@home.com&#39;: &#39;Robert Neff&#39;,\n &#39;david@djwhome.demon.co.uk&#39;: &#39;David Woolley&#39;,\n &#39;paul@ten-20.com&#39;: &#39;Paul Davis&#39;,\n &#39;rsv@retemail.es&#39;: &#39;Ricardo S?nchez&#39;,\n &#39;mcmay@w3.org&#39;: &#39;Matt May&#39;,\n &#39;joeclark@joeclark.org&#39;: &#39;Joe Clark&#39;,\n &#39;poehlman1@comcast.net&#39;: &#39;David Poehlman&#39;,\n &#39;hy@miplet.com&#39;: &#39;Hy Cohen&#39;,\n &#39;lsnider@thesnidersweb.com&#39;: &quot;The Snider&#39;s Web&quot;,\n &#39;josh@deaghean.com&#39;: &#39;Josh Hughes&#39;,\n &#39;anthonyq@testingcentre.com&#39;: &#39;Quinn, Anthony&#39;,\n &#39;foliot@wats.ca&#39;: &#39;John Foliot - WATS.ca&#39;,\n &#39;lkyoder@pacbell.net&#39;: &#39;Leslie K. Yoder&#39;,\n &#39;phark@phark.net&#39;: &#39;Mike Rundle&#39;,\n &#39;w3c@accessibleinter.net&#39;: &#39;Bill Mason&#39;,\n &#39;c.foster@umassp.edu&#39;: &#39;Carol Foster&#39;,\n &#39;kynn@idyllmtn.com&#39;: &#39;Kynn Bartlett&#39;,\n &#39;gerald.g.weichbrodt@ived.gm.com&#39;: &#39;Jerry Weichbrodt&#39;,\n &#39;carl.myhill@ps.ge.com&#39;: &#39;carl.myhill@ps.ge.com&#39;,\n &#39;jesper.tverskov@mail.tele.dk&#39;: &#39;Jesper Tverskov&#39;,\n &#39;matt@kbc.net.au&#39;: &#39;Matthew Smith&#39;,\n &#39;sdale@stevendale.com&#39;: &#39;Steven Dale&#39;,\n &#39;hfilipe@fe.up.pt&#39;: &#39;Helder Ferreira&#39;,\n &#39;tina@greytower.net&#39;: &#39;Tina Holmboe&#39;,\n &#39;GTWYcapitol@excite.com&#39;: &#39;Investor Relations&#39;,\n &#39;hanbei002@163.com&#39;: &#39;hanbei002@163.com&#39;,\n &#39;pjenkins@us.ibm.com&#39;: &#39;Phill Jenkins&#39;,\n &#39;hbingham@ACM.org&#39;: &#39;Harvey Bingham&#39;,\n &#39;asgilman@access.digex.net&#39;: &#39;Al Gilman&#39;,\n &#39;wai-report@w3.org&#39;: &#39;wai-report@w3.org&#39;,\n &#39;webmaster@google.com&#39;: &#39;Google Webmaster Autoresponder&#39;,\n &#39;h.climo@plymouth.ac.uk&#39;: &#39;Eddie Climo&#39;,\n &#39;Michelle@ngfl.gov.uk&#39;: &#39;Michelle Morris&#39;,\n &#39;chisholm@trace.wisc.edu&#39;: &#39;Wendy A Chisholm&#39;,\n &#39;nyree@flexicomm.com.au&#39;: &#39;Nyree Hibberd&#39;,\n &#39;richwine@bloomington.in.us&#39;: &#39;Brian Richwine&#39;,\n &#39;twilliams@ezgov.com&#39;: &#39;Tara Williams&#39;,\n &#39;chris.gibson@lokator.co.uk&#39;: &#39;Chris Gibson&#39;,\n &#39;bizdev@google.com&#39;: &#39;bizdev@google.com&#39;,\n &#39;dom@w3.org&#39;: &#39;Dominique Haza?l-Massieux&#39;,\n &#39;thomasholloway@earthlink.net&#39;: &#39;Tom Holloway&#39;,\n &#39;max.moritz.sievers@gmx.de&#39;: &#39;Max Moritz Sievers&#39;,\n &#39;MSemple@OfficeDepot.com&#39;: &#39;Marlene-Semple&#39;,\n &#39;David.John@company-net.com&#39;: &#39;David John&#39;,\n &#39;wai-xtech-request@tux.w3.org&#39;: &#39;wai-xtech-request@tux.w3.org&#39;,\n &#39;Irene.Vatton@inrialpes.fr&#39;: &#39;Irene.Vatton@inrialpes.fr&#39;,\n &#39;ve3ll@rac.ca&#39;: &#39;John Russell&#39;,\n &#39;brantgurganus2001@cherokeescouting.org&#39;: &#39;Brant Langer Gurganus&#39;,\n &#39;Steven.Pemberton@cwi.nl&#39;: &#39;Steven Pemberton&#39;,\n &#39;Laurent.Carcone@inrialpes.fr&#39;: &#39;Laurent Carcone&#39;,\n &#39;wildhack@albatros.cnb.net&#39;: &#39;Michael Wildhack &#39;,\n &#39;gerald@impressive.net&#39;: &#39;Gerald Oskoboiny&#39;,\n &#39;danbri@fireball.danbri.org&#39;: &#39;danbri@fireball.danbri.org&#39;,\n &#39;derhoermi@gmx.net&#39;: &#39;Bjoern Hoehrmann&#39;,\n &#39;jeff.mischkinsky@oracle.com&#39;: &#39;Jeff Mischkinsky&#39;,\n &#39;lematin@gmx.net&#39;: &#39;Fabian Weber&#39;,\n &#39;Scott_Boag@lotus.com&#39;: &#39;Scott Boag/CAM/Lotus&#39;,\n &#39;keshlam@us.ibm.com&#39;: &#39;keshlam@us.ibm.com&#39;,\n &#39;onime@ictp.trieste.it&#39;: &#39;ONIME EHIMIKA OHIREIME &#39;,\n &#39;instrument@vsnl.net&#39;: &#39;TradeMakers&#39;,\n &#39;easytest8341@email.com&#39;: &#39;easytest8341@email.com&#39;,\n &#39;olive@starlink.ru&#39;: &#39;Olive&#39;,\n &#39;wgq@armstrong.com.cn&#39;: &#39;wgq \\uf8e8&#39;,\n &#39;Ghoraeian@majlis.ir&#39;: &#39;Majlis&#39;,\n &#39;contrcc@cayo.cco.cyt.cu&#39;: &#39;contrcc&#39;,\n &#39;eperez@mag.enap.cl&#39;: &#39;Perez Barrientos, Enrique (Mag)&#39;,\n &#39;AndrewWatt2001@aol.com&#39;: &#39;AndrewWatt2001@aol.com&#39;}"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "email_name = {}\n",
    "for value in mail_df.values:\n",
    "    email_name[value[6]] = value[5]\n",
    "email_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDBにmailテーブルを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RDBにmail_dfのテーブルを作成する\n",
    "from db import connect\n",
    "engine = connect()\n",
    "canonical_mail_df.to_sql(name='mail',con=engine,if_exists='replace',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下の2つに関する処理\n",
    "  1. メンションとEntityを対応させる辞書をつくる（entity_dict）\n",
    "  2. Entityテーブルのタプルとなるentity_rowsを作る（属性：message_id, entity）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mail_dfのヘッダー（from, date）から，メールアドレス，送信者，送受信日を取得する\n",
    "\n",
    "entity_rows = [] #Entityテーブルの行を格納するリスト\n",
    "entity_dict = {} \n",
    "for values in mail_df.values:\n",
    "    message_id = values[0]\n",
    "    date = values[3]\n",
    "    sender = values[5] # 送信者\n",
    "    address = values[6] # 送信者のメールアドレス\n",
    "    date = re.search(r'(\\d{1,2} \\w{3} \\d{2,4})', date) # 送信日\n",
    "    date = date.group()\n",
    "    # 辞書に登録する\n",
    "    entity_dict.setdefault(address,{address:\"MAIL\"})\n",
    "    entity_dict.setdefault(sender,{sender:\"PERSON\"})\n",
    "    entity_dict.setdefault(date,{date:\"DATE\"})\n",
    "    # 行に追加する\n",
    "    entity_rows.append((message_id, address))\n",
    "    entity_rows.append((message_id, sender))\n",
    "    entity_rows.append((message_id, date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bodyに含まれる文をEntity linkerにかけてEntityを取得する\n",
    "# entityLinkingFromBody.ipynb"
   ]
  },
  {
   "source": [
    "# mail_dfのsubjectをEntity linkerにかけてEntityを取得する．\n",
    "total_start = time.time()\n",
    "count = 0\n",
    "\n",
    "for values in mail_df.values:\n",
    "    subject = values[6]\n",
    "    subject = re.sub(r'(\\[.+\\] )','',subject)\n",
    "    subject = re.sub(r'(\\n\\t)',' ',subject)\n",
    "    subject = re.sub(r'\\n{1,}',' ',subject)\n",
    "    subject = re.sub(r'\\t{1,}',' ',subject)\n",
    "    \n",
    "    part_start = time.time()\n",
    "\n",
    "    json_res = tagme(subject)\n",
    "    linked_entities = [annotation for annotation in json_res['annotations'] if annotation['rho'] > 0.3]\n",
    "    if linked_entities != []:\n",
    "        for le in linked_entities:\n",
    "            spot = le['spot']\n",
    "            entity_dict.setdefault(spot,{le['title']:le['id']}) # 辞書に登録する\n",
    "            entity_rows.append((values[1],le['title'])) # 行に追加する\n",
    "\n",
    "    count = count + 1\n",
    "    print(\"Processing:{}%\".format((count/len(bodies)) * 100))\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(\"Total time:{} minutes\".format(total_time/60))"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "source": [
    "import pickle\n",
    "\n",
    "with open('/Users/taroaso/myprojects/OpenIE/trec/output/entity_dict.pickle', mode='wb') as f:\n",
    "    pickle.dump(entity_dict,f)\n",
    "\n",
    "with open('/Users/taroaso/myprojects/OpenIE/trec/output/entity_rows.pickle', mode='wb') as f:\n",
    "    pickle.dump(entity_rows,f)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新規Entity候補の抽出\n",
    "  * NERによって，Named Entityを抽出し，entity_dictに登録がなければ，新規Entity候補として，別の辞書に登録する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Entity_dictに登録のない新規エンティティ候補数:212\n"
    }
   ],
   "source": [
    "# 新規エンティティ候補を登録する辞書をentity_candidate_dictとし，行をentity_candidate_rowsとする\n",
    "entity_candidate_dict = {}\n",
    "entity_candidate_rows = []\n",
    "for mail in mail_df.values:\n",
    "    body = mail[-1]\n",
    "    #body = re.sub(r'\\n{2,}','\\n',mail['body'])\n",
    "    #body = re.sub(r'\\n{1}',' ',body)\n",
    "    #body = re.sub(r'( >){1,}','',body)\n",
    "    #body = re.sub(r'\\*{1,}','',body)\n",
    "    #body = re.sub(r'(On)','. On',body)\n",
    "    doc = nlp(body)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ not in ('DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'CARDINAL', 'ORDINAL'):\n",
    "            ent = ent.to_dict()\n",
    "            if ent[\"text\"] in list(entity_dict.keys()):\n",
    "                pass\n",
    "            else:\n",
    "                entity_candidate_dict[ent[\"text\"]] = ent[\"type\"]\n",
    "                entity_candidate_rows.append((mail['message_id'],ent[\"text\"]))\n",
    "print('Entity_dictに登録のない新規エンティティ候補数:{}'.format(len(entity_candidate_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新規エンティティ候補（メンション）をentity_dictに登録されているメンションと名寄せする（類似のメンションを検出する）\n",
    "* 名寄せできたメンションとエンティティの辞書をつくる（add_entity_dict）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "entity_dictと名寄せできたメンション数:43\n"
    }
   ],
   "source": [
    "# 名寄せ\n",
    "add_entity_dict = {}\n",
    "\n",
    "for candidate in entity_candidate_dict.keys():\n",
    "    threshold = 0.5\n",
    "    candidate_tokens = candidate.split()\n",
    "    filtered_candidate_tokens = [token for token in candidate_tokens if token not in stop_words]\n",
    "    filtered_candidate_tokens = set(list(map(str.lower, filtered_candidate_tokens)))\n",
    "    for spot in entity_dict.keys():\n",
    "        # jaccard distance\n",
    "        spot_tokens= spot.split()\n",
    "        filtered_spot_tokens = [token for token in spot_tokens if token not in stop_words]\n",
    "        filtered_spot_tokens = set(list(map(str.lower,filtered_spot_tokens)))\n",
    "        jd = jaccard_distance(filtered_candidate_tokens, filtered_spot_tokens)\n",
    "        # edit distance\n",
    "        filtered_spot = ' '.join(filtered_spot_tokens)\n",
    "        filtered_candidate = ' '.join(filtered_candidate_tokens)\n",
    "        ed = edit_distance(filtered_spot, filtered_candidate)/max(len(filtered_spot),len(filtered_candidate))\n",
    "        if min(jd,ed) < threshold:\n",
    "            threshold = min(jd,ed)\n",
    "            add_entity_dict[candidate] = entity_dict[spot]\n",
    "print('entity_dictと名寄せできたメンション数:{}'.format(len(add_entity_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新規エンティティ候補内で名寄せを行い，その中の1つにマッピングする\n",
    "* マッピングした辞書をmapped_ne_dictとする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entity_candidate_list = list(entity_candidate_dict.keys() - add_entity_dict.keys())\n",
    "entity_candidate_list_lower = list(map(str.lower, entity_candidate_list))\n",
    "similar_dict = {}\n",
    "for i, ne1 in enumerate(entity_candidate_list_lower):\n",
    "    ne1_tokens = ne1.split()\n",
    "    ne1_tokens = [token for token in ne1_tokens if token not in stop_words]\n",
    "    ne1_strings = ' '.join(ne1_tokens)\n",
    "    similar_ne = [] # 類似するentity candidateをまとめる\n",
    "    for j, ne2 in enumerate(entity_candidate_list_lower):\n",
    "        # jaccard distance\n",
    "        ne2_tokens = ne2.split()\n",
    "        ne2_tokens = [token for token in ne2_tokens if token not in stop_words]\n",
    "        jd = jaccard_distance(set(ne1_tokens), set(ne2_tokens))\n",
    "        # edit distance\n",
    "        ne2_strings = ' '.join(ne2_tokens)\n",
    "        ed = edit_distance(ne1_strings, ne2_strings)/max(len(ne1_strings),len(ne2_strings))\n",
    "        if min(jd,ed) < 0.4:\n",
    "            similar_ne.append(entity_candidate_list[j])\n",
    "    similar_dict[entity_candidate_list[i]] = similar_ne\n",
    "\n",
    "mapped_ne_dict = {}\n",
    "for key, value_list in similar_dict.items():\n",
    "    length_list = list(map(lambda x:len(x), value_list))\n",
    "    idx = length_list.index(min(length_list))\n",
    "    mapped_ne_dict[key] = {value_list[idx]:entity_candidate_dict[value_list[idx]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3つの辞書（entity_dict, add_entity_dict, mapped_ne_dict）を統合する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "integrated_entity_dict = dict(**entity_dict,**add_entity_dict,**mapped_ne_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wiki-research-l/output/integrated_entity_dict.pkl\",\"wb\") as f:\n",
    "    pickle.dump(integrated_entity_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"wiki-research-l/output/integrated_entity_dict.pkl\",\"rb\") as f:\n",
    "    integrated_entity_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entity_candidate_rowsの新規エンティティ候補（メンション）を辞書と照合し，対応するエンティティと置き換え，entity_rowsに追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in entity_candidate_rows:\n",
    "    mention = row[1]\n",
    "    entity = list(integrated_entity_dict[mention].keys())[0]\n",
    "    row = (row[0], entity)\n",
    "    entity_rows.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDBにEntityテーブルを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDBにmail_dfのテーブルを作成する\n",
    "entity_df = pd.DataFrame(entity_rows, columns=['message_id','entity'])\n",
    "\n",
    "from db import connect\n",
    "engine = connect()\n",
    "\n",
    "entity_df.to_sql(name='wiki_research_l_entity',con=engine,if_exists='replace',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 代名詞YOUに関する辞書\n",
    "refer_you = {}\n",
    "for mail in bodies:\n",
    "    message_id = mail['message_id']\n",
    "    # triple中の代名詞youの候補の辞書を作るために，Greetingsの行からYouの候補を取り出す．\n",
    "    for greetings in mail['greetings']:\n",
    "        if greetings != []:\n",
    "            doc = nlp(greetings[0])\n",
    "            for ent in doc.ents:\n",
    "                if ent.type in ['PERSON']:\n",
    "                    refer_you[message_id] = {\"YOU\":ent.text, \"YOUR\":ent.text + '\\'s'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# その他の代名詞I, MY, ME, WE, OUR, USに関する辞書\n",
    "refer_pronoun = {}\n",
    "for values in mail_df.values:\n",
    "    message_id = values[1]\n",
    "    sender = values[2]\n",
    "    start = re.search(r'(\\(.+\\))',sender).start()\n",
    "    end = re.search(r'(\\(.+\\))',sender).end()\n",
    "    sender = sender[start+1:end-1]\n",
    "    refer_pronoun[message_id]={'I':sender, 'MY':sender + '\\'s', 'ME':sender, 'WE':sender, 'OUR':sender + '\\'s', 'US':sender}\n",
    "\n",
    "# 2つの辞書を結合する\n",
    "for key, value in refer_you.items():\n",
    "    if key in refer_pronoun:\n",
    "        d = refer_pronoun[key]\n",
    "        d.update(value)\n",
    "        refer_pronoun[key] = d\n",
    "    else:\n",
    "        refer_pronoun[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文章からトリプルを抽出する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence_list = []\n",
    "for mail in bodies:  #1通ずつ取り出す\n",
    "    text = ''\n",
    "    # 1行{行番号:文}ずつ取り出し，複数文が含まれた1つの文の連なりにする\n",
    "    for sentence in mail['sentence']: \n",
    "        text = text + list(sentence.values())[0] + ' '\n",
    "    # 文章を文に分解する\n",
    "    if text == '':\n",
    "        pass\n",
    "    else:\n",
    "        doc = nlp(text)\n",
    "        for sentence in doc.sentences:\n",
    "            # 単語のlemmatizationを辞書にする\n",
    "            lemma = {}\n",
    "            for word in sentence.words:\n",
    "                lemma[word.text] = word.lemma\n",
    "            sentence_list.append((mail['message_id'], sentence.text, lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total time:0.5135371287663778 minutes\n"
    }
   ],
   "source": [
    "total_start = time.time()\n",
    "# MinIEにかける\n",
    "import requests\n",
    "import json\n",
    "\n",
    "extractions_list = []\n",
    "for tpl in sentence_list:\n",
    "    message_id = tpl[0]\n",
    "    sentence = tpl[1].encode('utf-8')\n",
    "    lemma = tpl[-1]\n",
    "    try:\n",
    "        response = requests.post('http://localhost:8080/minie/query', data=sentence)\n",
    "        result = response.json()\n",
    "        if result['facts'] == []:\n",
    "            pass\n",
    "        else:\n",
    "            for triple in result['facts']:\n",
    "                sbj = triple['subject']\n",
    "                obj = triple['object']\n",
    "                lemma_predicate = triple['predicate']\n",
    "#                token_list = triple['predicate'].split()\n",
    "#                lemma_list = []\n",
    "#                for token in token_list:\n",
    "#                    try:\n",
    "#                        lemma_list.append(lemma[token])\n",
    "#                    except:\n",
    "#                        lemma_list.append(token)\n",
    "#                lemma_predicate = ' '.join(lemma_list)\n",
    "                extractions_list.append([message_id, sentence.decode('utf-8'), sbj, lemma_predicate, obj])\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "# dataframeにする\n",
    "triple_df = pd.DataFrame(extractions_list, columns = ['message_id','sentence', 'subject', 'predicate', 'object'])\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(\"Total time:{} minutes\".format(total_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トリプルを代名詞の辞書と照合し，代名詞が含まれていれば対応するエンティティで置き換える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Senderに置き換えるための辞書を使って実際に置き換える\n",
    "triples = []\n",
    "for row in triple_df.values:\n",
    "    message_id = row[0]\n",
    "    sentence = row[1]\n",
    "    sbj = row[2].split()\n",
    "    pred = row[3].split()\n",
    "    obj = row[4].split()\n",
    "    pronouns = refer_pronoun[message_id]\n",
    "    # subjectの置き換え\n",
    "    for i, word in enumerate(sbj):\n",
    "        entity = pronouns.get(word.upper())\n",
    "        if entity is None:\n",
    "            continue\n",
    "        else:\n",
    "            sbj[i] = entity\n",
    "    sbj = ' '.join(sbj)\n",
    "    # predicateの置き換え\n",
    "    for i, word in enumerate(pred):\n",
    "        entity = pronouns.get(word.upper())\n",
    "        if entity is None:\n",
    "            continue\n",
    "        else:\n",
    "            pred[i] = entity\n",
    "    pred = ' '.join(pred)\n",
    "    # objectの置き換え\n",
    "    for i, word in enumerate(obj):\n",
    "        entity = pronouns.get(word.upper())\n",
    "        if entity is None:\n",
    "            continue\n",
    "        else:\n",
    "            obj[i] = entity\n",
    "    obj = ' '.join(obj)\n",
    "    # [new_arg1, new_rel, new_arg2s]を1行として追加\n",
    "    triples.append((message_id, sentence, sbj, pred, obj))"
   ]
  },
  {
   "source": [
    "# トリプルの主語，目的語が代名詞のみの場合，そのトリプルを除外（削除）する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# トリプルの述語に\"...\"が含まれている場合，そのトリプルを削除する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# トリプルの述語の一部をlemmatizationする"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トリプルとEntityの辞書を照合し，辞書に登録されているEntityで置き換える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_triples = []\n",
    "mentions = list(integrated_entity_dict.keys())\n",
    "for row in triples:\n",
    "    message_id = row[0]\n",
    "    sentence = row[1]\n",
    "    sbj = row[2]\n",
    "    pred = row[3]\n",
    "    obj = row[4]\n",
    "\n",
    "    # subjectあるいはobjectが辞書に登録されているメンションと一致する場合\n",
    "    if sbj in mentions:\n",
    "        sbj = list(integrated_entity_dict[sbj].keys()) #メンションに対応するEntityで置き換える\n",
    "        sbj = sbj[0]\n",
    "        canonical_triples.append((message_id, sentence, sbj, pred, obj))\n",
    "    elif obj in mentions:\n",
    "        obj = list(integrated_entity_dict[obj].keys()) #メンションに対応するEntityで置き換える\n",
    "        obj = obj[0]\n",
    "        canonical_triples.append((message_id, sentence, sbj, pred, obj))\n",
    "    else:\n",
    "        for mention in mentions:\n",
    "            entity = list(integrated_entity_dict[mention].keys())\n",
    "            entity = entity[0]\n",
    "            if re.search(re.escape(mention), sbj):\n",
    "                canonical_triples.append((message_id, sentence, sbj, pred, obj))\n",
    "                canonical_triples.append((message_id, sentence, entity, 'seeAlso', sbj))\n",
    "            elif re.search(re.escape(mention), obj):\n",
    "                canonical_triples.append((message_id, sentence, sbj, pred, obj))\n",
    "                canonical_triples.append((message_id, sentence, entity, 'seeAlso', obj))\n",
    "\n",
    "canonical_triple_df = pd.DataFrame(canonical_triples, columns=['message_id', 'sentence', 'sbject', 'predicate', 'object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDBにtripleテーブルを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db import connect\n",
    "engine = connect()\n",
    "\n",
    "canonical_triple_df = canonical_triple_df.drop_duplicates()\n",
    "canonical_triple_df.to_sql(name='wiki_research_l_triple',con=engine,if_exists='replace',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['has', 'be great suggestion to include', 'seeAlso', 'be include',\n       'be familiar with', 'love to interview you for',\n       'love test visualization for', 'love to interview', 'love',\n       'schedule', 'be look for researcher so unfortunately in',\n       'be look for developer so unfortunately in', 'be look for', 'be',\n       'be to bring together',\n       'be to bring together QUANT_O_1 party work toward',\n       'invite submission of', 'talk',\n       'organize experience Wikipedians to thank thousand of',\n       'find that receive in', 'find that',\n       'increase QUANT_R_1 week retention by',\n       'increase QUANT_R_1 week retention on', 'increase', 'look at',\n       'do find', 'feel', 'has effort on',\n       'make valuable discovery about who spend time support other because of',\n       'value', 'move pre-prints toward submission for', 'move', 'is',\n       'has team of', 'be grateful to',\n       'partner with J. Nathan Matias in', 'partner with', 'has in',\n       'work with', 'work in design of', 'work',\n       'be do in collaboration with', 'be co-create by',\n       'be work to open process & software to wider range of community in',\n       'be work to open process & software to wider range of researcher in',\n       'be work to open process & software in', 'be work to open',\n       'be building on workshop QUANT_O_1 > in',\n       'be building on community research summit QUANT_O_1 > in',\n       'be develop idea for next round of collaboration toward',\n       'be develop fundraising toward', 'be develop',\n       'be develop idea for next round of collaboration',\n       'be develop fundraising', 'has next round of',\n       'have start new Formal collaboration with',\n       'have start new Formal collaboration with University of Turin on',\n       'is in', 'have engagement with Images in',\n       'be main formal collaborator', 'be thankful to', 'be thankful for',\n       'aim to keep', 'link to', 'be capture', 'be point of',\n       'be neuroscience researcher at',\n       'be interested in explore biase on', 'hope to analyze', 'identify',\n       'to see actually', 'to see', 'need to be', 'do intend to request',\n       'be live-streamed on Wednesday at', 'be live-streamed on',\n       'be QUANT_R_1 of most important online resource for',\n       'have be highlight during', 'have be create',\n       'get better understanding of state of medical knowledge in',\n       \"give overview on how Wikipedia 's health content be use by different audience in\",\n       'be use by', 'join conversation on irc at',\n       'be most access web site for', 'engage with', 'have',\n       'be QUANT_R_1 of', 'have be publish online at', 'have be publish',\n       'be base on', 'to rely on',\n       'assess coverage of COVID-19-related research in Wikipedia via',\n       'assess', 'be integrate new research at', 'be integrate',\n       'be represent from', 'figure out', 'regretfully do have',\n       'be target', 'suggest', '-LSB- send -RSB- detail to',\n       '-LSB- send -RSB- directly', '-LSB- send -RSB-', 'post detail to',\n       'post', 'look to read experimental design take factor into',\n       'look',\n       'have article on QUANT_R_1 female fellow of Royal Society for',\n       'achieve', 'have article on QUANT_R_1 Nobel Prize Winner by',\n       'have article on', 'be ask to restore', 'tell they to start from',\n       'have develop to investigate computer base system for',\n       'have develop', 'be register for', 'be available under', 'be in',\n       'reach out to legal to inquire about',\n       'reach out at wikimedia.org to inquire about',\n       'reach out to inquire about', 'try to parse', 'need to go back in',\n       'need to go back longer than', 'need to go back as', 'need to go',\n       'be list at', 'show you next set of', 'prefer', 'have look at',\n       'cover', 'have have have take place', 'be geolocate',\n       'be geolocate ip address in', 'be QUANT_O_1 most important for',\n       'be QUANT_R_1 of deletion process QUANT_O_1 most important for',\n       'be QUANT_R_1 day process for',\n       'be surprised if article be delete per A7 that',\n       'be surprised that', 'have be delete per', 'expect that to find',\n       'be happen on', 'be available knowledge base host by',\n       'be host by',\n       'have see increase in number of publication around Wikidata in',\n       'have see', 'hope to provide', 'address challenge of',\n       'encourage range of', 'encourage', 'encourage description of',\n       'be link to', \"'re\",\n       'feed for instance by improve on QUANT_R_1 wikidata aspect back into',\n       'feed for instance by comment back into',\n       'feed for instance by suggest new design feature back into',\n       'feed for instance by suggest tool back into',\n       'feed for instance by suggest practice back into',\n       'encourage submission on', 'is natural language generation by',\n       'is abstract representation of', 'welcome', 'discuss area of',\n       'shed light on benefit of', 'be consider online option in',\n       'be consider', 'welcome follow type of', 'be as',\n       \"'m with researcher at\", 'be work on identify missing content on',\n       'speak', 'be compensate for', 'be work on', 'be work as part of',\n       'be work', 'have start',\n       'have start new Formal collaboration with team from École Polytechnique Fédérale de Lausanne to work collaboratively on',\n       'contribute to program', 'contribute', 'be to those of you to',\n       'be to those of you for', 'be to those of', 'continue',\n       'have work with', 'be shape proposal for', 'act as',\n       'be program approach to analyze',\n       'be perform by ip address out of', 'be perform by', 'be miss',\n       'be aware of', 'map wikidata qid to', 'map', 'have be use',\n       'write up quickly', 'publish', 'write up', 'need to generate',\n       'do read response', 'do read', 'invite',\n       'teach you most unexpected thing about', 'be new to', 'learn',\n       'have parse', 'make dev table publicly available on',\n       'be commit to', \"'re prepare for\", 'look for',\n       'be rise ... it give good model for decline editor number for',\n       'be decline ... it give good model for decline editor number for',\n       'be rise ... it give personally good model for',\n       'be decline ... it give personally good model for',\n       'be rise ... it give good model for',\n       'be decline ... it give good model for',\n       'it be ... it give good model for decline editor number for',\n       'it be ... it give personally good model for',\n       'it be ... it give good model for',\n       'it be rise ... give good model for decline editor number for',\n       'it be decline ... give good model for decline editor number for',\n       'it be rise ... give personally good model for',\n       'it be decline ... give personally good model for',\n       'it be rise ... give good model for',\n       'it be decline ... give good model for', 'have be influential in',\n       'have be', 'view Wikipedia', 'view'], dtype=object)"
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "source": [
    "canonical_triple_df['predicate'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}