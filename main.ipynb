{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, email\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pprint, re, time\n",
    "\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loadFile import getFileList, getDirList, fileToDataFrame\n",
    "\n",
    "# ディレクトリ 内のメールファイルを読み込む\n",
    "directory_path = \"wiki-research-l/2020-July\"\n",
    "file_list = getFileList(directory_path)\n",
    "file_list.sort()\n",
    "\n",
    "# テキストファイルをデータフレームに格納する\n",
    "mail_df, thread_df = fileToDataFrame(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メールのBody部分を各パートに分解する\n",
    "bodies = {dict(Bodies=[])}\n",
    "for idx, body in mail_df.loc[:,'Body'].items():\n",
    "    origin = []\n",
    "    greetings = dict(Greetings=[])\n",
    "    sentences = dict(Sentence=[])\n",
    "    captions = dict(Caption=[])\n",
    "    bulletlist = dict(Bulletlist=[])\n",
    "    ending = dict(Ending=[])\n",
    "    quotation = dict(Quotation=[])\n",
    "    footer = dict(Footer=[])\n",
    "    misc = dict(Misc=[])\n",
    "\n",
    "    lines = body.splitlines()\n",
    "    for num, line in enumerate(lines):\n",
    "        if re.match(r'\\[G\\]',line) is not None:\n",
    "            greetings['Greetings'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[S\\]',line) is not None:\n",
    "            sentences['Sentence'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[C\\]',line) is not None:\n",
    "            captions['Caption'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[B\\]',line) is not None:\n",
    "            bulletlist['Bulletlist'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[E\\]',line) is not None:\n",
    "            ending['Ending'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[Q\\]',line) is not None:\n",
    "            quotation['Quotation'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[F\\]',line) is not None:\n",
    "            footer['Footer'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        elif re.match(r'\\[M\\]',line) is not None:\n",
    "            misc['Misc'].append({num:line[3:]})\n",
    "            origin.append(line[3:])\n",
    "        else: #空白行に対応する\n",
    "            continue\n",
    "    originbody = ' '.join(origin)\n",
    "    bodies['Bodies'].append({'idx':idx, 'countrows':len(lines), 'body':originbody, **greetings, **sentences, **captions, **bulletlist, **ending, **quotation, **footer, **misc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#　bodiesのsentenceのvalueから複数文のテキストを作る\n",
    "s_list = []\n",
    "for i in bodies['Bodies']:  #1通ずつ取り出す\n",
    "    text = ''\n",
    "    for j in i['Sentence']: #1行{行番号:文}ずつ取り出し，複数文が含まれた1つの文字列にする\n",
    "        text = text + list(j.values())[0] + ' '\n",
    "    s_list.append(text) #1通ごとの自然文のテキストをリストに格納する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# センテンスのdataframeを作る\n",
    "sentence_list = []\n",
    "# 文章全体に対する前処理\n",
    "for i, content in enumerate(s_list):\n",
    "    sentences = tokenize.sent_tokenize(content)\n",
    "    # 文に対する前処理\n",
    "    for j, sentence in enumerate(sentences):\n",
    "        sentence = re.sub(r'\\s{2,}',' ',sentence)\n",
    "        sentence_list.append([mail_df['Message-ID'][i],sentence])\n",
    "sentence_df = pd.DataFrame(sentence_list,columns=['Message-ID','sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                             Message-ID  \\\n0  <CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com>   \n1  <CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com>   \n2  <CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com>   \n3  <CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com>   \n4  <CAE4fJj_b7k8yeqmz19a-seo3gtL9Mg1nvQO0oPUtrxvCFMkEaw@mail.gmail.com>   \n\n                                                                                                          sentence  \n0                                                                                           Thanks for your reply!  \n1  It's a great suggestion to include a list of ORES based tools for people who are not familiar with ORES itself.  \n2                                    For our research, we would love to interview you and test our visualizations.  \n3                               Please let me know what times will work best for you so we can schedule a meeting.  \n4                                                                                           Thanks for clarifying.  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Message-ID</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com&gt;</td>\n      <td>Thanks for your reply!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com&gt;</td>\n      <td>It's a great suggestion to include a list of ORES based tools for people who are not familiar with ORES itself.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com&gt;</td>\n      <td>For our research, we would love to interview you and test our visualizations.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com&gt;</td>\n      <td>Please let me know what times will work best for you so we can schedule a meeting.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;CAE4fJj_b7k8yeqmz19a-seo3gtL9Mg1nvQO0oPUtrxvCFMkEaw@mail.gmail.com&gt;</td>\n      <td>Thanks for clarifying.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "#sentence_df.to_csv('wiki-research-l/output/sentence_df.csv')\n",
    "sentence_df = pd.read_csv('/Users/taroaso/myprojects/OpenIE/wiki-research-l/output/sentence_df.csv',index_col=0)\n",
    "sentence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('It', 'PRP'), (\"'s\", 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('suggestion', 'NN'), ('to', 'TO'), ('include', 'VB'), ('a', 'DT'), ('list', 'NN'), ('of', 'IN'), ('ORES', 'NNP'), ('based', 'VBN'), ('tools', 'NNS'), ('for', 'IN'), ('people', 'NNS'), ('who', 'WP'), ('are', 'VBP'), ('not', 'RB'), ('familiar', 'JJ'), ('with', 'IN'), ('ORES', 'NNP'), ('itself', 'PRP'), ('.', '.')]\n"
    }
   ],
   "source": [
    "words = [nltk.word_tokenize(sent) for sent in sentence_df['sentence']]\n",
    "pos_words = [nltk.pos_tag(word) for word in words]\n",
    "chunk_words = [nltk.chunk.ne_chunk(pos_word, binary=True) for pos_word in pos_words]\n",
    "print(pos_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('CAT', 'NNP'),\n ('Lab', 'NNP'),\n ('<', 'NNP'),\n ('https', 'NN'),\n (':', ':'),\n ('//citizensandtech.org/', 'NN'),\n ('>', 'NN'),\n ('has', 'VBZ'),\n ('partnered', 'VBN'),\n ('with', 'IN'),\n ('Arabic', 'NNP'),\n (',', ','),\n ('German', 'NNP'),\n (',', ','),\n ('Persian', 'NNP'),\n ('and', 'CC'),\n ('Polish', 'JJ'),\n ('language', 'NN'),\n ('Wikipedias', 'NNP'),\n ('to', 'TO'),\n ('answer', 'VB'),\n ('those', 'DT'),\n ('questions', 'NNS'),\n ('-', ':'),\n ('and', 'CC'),\n ('more', 'JJR'),\n ('-', ':'),\n ('in', 'IN'),\n ('two', 'CD'),\n ('new', 'JJ'),\n ('studies', 'NNS'),\n ('.', '.')]"
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "source": [
    "pos_words[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "are not familiar with\nORES\n"
    }
   ],
   "source": [
    "rel = nltk.word_tokenize('are not')\n",
    "arg2s = nltk.word_tokenize('familiar with ORES')\n",
    "#test = [nltk.pos_tag(t) for t in test] \n",
    "#test = [nltk.chunk.ne_chunk(t, binary=True) for t in test]\n",
    "\n",
    "cnt = 0\n",
    "for word in arg2s:\n",
    "    idx = words[1].index(word)\n",
    "    pos = pos_words[1][idx][1]\n",
    "    if pos in ('IN', 'TO', 'JJ'):\n",
    "        rel.append(word)\n",
    "        cnt = cnt + 1\n",
    "    else:\n",
    "        break\n",
    "del arg2s[:cnt]\n",
    "rel = ' '.join(rel)\n",
    "arg2s = ' '.join(arg2s)\n",
    "print(rel)\n",
    "print(arg2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['familiar', 'with', 'ORES'] ['JJ', 'IN', 'NNP']\n['a', 'list', 'of', 'ORES', 'based', 'tools', 'for', 'people'] ['DT', 'NN', 'IN', 'NNP', 'VBN', 'NNS', 'IN', 'NNS']\n['a', 'great', 'suggestion', 'to', 'include', 'a', 'list', 'of', 'ORES', 'based', 'tools', 'for', 'people'] ['DT', 'JJ', 'NN', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNP', 'VBN', 'NNS', 'IN', 'NNS']\n['our', 'visualizations'] ['PRP$', 'NNS']\n['to', 'test', 'our', 'visualizations', 'For', 'our', 'research'] ['TO', 'VB', 'PRP$', 'NNS', 'IN', 'PRP$', 'NN']\n['you'] ['PRP']\n['to', 'interview', 'you', 'For', 'our', 'research'] ['TO', 'NN', 'PRP', 'IN', 'PRP$', 'NN']\n['a', 'meeting'] ['DT', 'NN']\n['familiar', 'with', 'ORES'] ['JJ', 'IN', 'NNP']\n['for', 'developers', 'in', 'this', 'research'] ['IN', 'NNS', 'IN', 'DT', 'NN']\n['ORES'] ['NN']\n['familiar', 'with', 'ORES'] ['JJ', 'IN', 'NNP']\n['for', 'researchers', 'in', 'this', 'research'] ['IN', 'NNS', 'IN', 'DT', 'NN']\n['to', 'bring', 'together', 'all', 'parties', 'now', 'in', 'its', 'second', 'edition'] ['TO', 'VB', 'RB', 'DT', 'NNS', 'RB', 'IN', 'PRP$', 'JJ', 'NN']\n['toward', 'improving', 'trustworthiness', 'of', 'online', 'communications'] ['IN', 'VBG', 'NN', 'IN', 'NN', 'NNS']\n['toward', 'improving', 'the', 'truthfulness', 'of', 'online', 'communications'] ['IN', 'VBG', 'DT', 'NN', 'IN', 'NN', 'NNS']\n['to', 'bring', 'together', 'all', 'parties', 'now', 'in', 'its', 'second', 'edition'] ['TO', 'VB', 'RB', 'DT', 'NNS', 'RB', 'IN', 'PRP$', 'JJ', 'NN']\n['social', 'media', 'platforms', 'on', 'the', 'following', 'indicative', 'list', 'of', 'topics'] ['JJ', 'NNS', 'NNS', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'NNS']\n['submissions', 'of', 'both', 'technical', 'papers'] ['NNS', 'IN', 'DT', 'JJ', 'NNS']\n['those', 'questions'] ['DT', 'NNS']\n['with', 'Polish', 'language', 'Wikipedias', 'to', 'answer', 'those', 'questions', '-', 'and', 'more', '-', 'in', 'two', 'new', 'studies'] ['IN', 'JJ', 'NN', 'NNP', 'TO', 'VB', 'DT', 'NNS', ':', 'CC', 'JJR', ':', 'IN', 'CD', 'JJ', 'NNS']\n['those', 'questions'] ['DT', 'NNS']\n['with', 'Persian', 'language', 'Wikipedias', 'to', 'answer', 'those', 'questions', 'more', '-', 'in', 'two', 'new', 'studies'] ['IN', 'JJ', 'NN', 'NNP', 'TO', 'VB', 'DT', 'NNS', 'JJR', ':', 'IN', 'CD', 'JJ', 'NNS']\n['those', 'questions'] ['DT', 'NNS']\n['with', 'German', 'language', 'Wikipedias', 'to', 'answer', 'those', 'questions', 'more', '-', 'in', 'two', 'new', 'studies'] ['IN', 'JJ', 'NN', 'NNP', 'TO', 'VB', 'DT', 'NNS', 'JJR', ':', 'IN', 'CD', 'JJ', 'NNS']\n['those', 'questions'] ['DT', 'NNS']\n['with', 'Arabic', 'language', 'Wikipedias', 'to', 'answer', 'those', 'questions', '-', 'and', 'more', '-', 'in', 'two', 'new', 'studies'] ['IN', 'NNP', 'NN', 'NNP', 'TO', 'VB', 'DT', 'NNS', ':', 'CC', 'JJR', ':', 'IN', 'CD', 'JJ', 'NNS']\n['two', 'week', 'retention', 'by', '2', 'percentage', 'points', '*', 'on', 'average'] ['CD', 'NN', 'NN', 'IN', 'CD', 'NN', 'NNS', 'VBP', 'IN', 'NN']\n['that', '*', 'receiving', 'a', 'Thanks', 'increased', 'two', 'week', 'retention', 'by', '2', 'percentage', 'points', '*', 'on', 'average', 'In', 'a', 'field', 'experiment'] ['DT', 'VBZ', 'VBG', 'DT', 'NNS', 'VBD', 'CD', 'NN', 'NN', 'IN', 'CD', 'NN', 'NNS', 'VBP', 'IN', 'NN', 'IN', 'DT', 'NN', 'NN']\n['thousands', 'of', 'editors'] ['NNS', 'IN', 'NNS']\n['Wikipedians'] ['NNS']\n['43', '%', 'more', 'thanks', 'on', 'average'] ['CD', 'NN', 'JJR', 'NNS', 'IN', 'NN']\n['recipients', 'to', 'send', '43', '%', 'more', 'thanks', 'on', 'average'] ['NNS', 'TO', 'VB', 'CD', 'NN', 'JJR', 'NNS', 'IN', 'NN']\n['at', 'the', 'effects', 'on', 'senders', 'of', 'giving', 'Thanks', '<', 'https', ':', '//citizensandtech.org/2020/06/mentoring-thanking-and-burnout-wikipedia/'] ['IN', 'DT', 'NNS', 'IN', 'NNS', 'IN', 'VBG', 'NNP', 'NNP', 'NN', ':', 'JJ']\n['from', 'their', 'efforts'] ['IN', 'PRP$', 'NNS']\n['emotionally', 'drained', 'from', 'their', 'efforts', 'already'] ['RB', 'VBN', 'IN', 'PRP$', 'NNS', 'RB']\n['because', '*', 'many', '*', '*', 'volunteers', 'already', 'felt', 'emotionally', 'drained', 'from', 'their', 'efforts', 'were', \"n't\", 'able', 'to', 'complete', 'the', 'study', '*'] ['IN', 'NNP', 'JJ', 'NNP', 'NNP', 'NNS', 'RB', 'VBD', 'RB', 'VBN', 'IN', 'PRP$', 'NNS', 'VBD', 'RB', 'JJ', 'TO', 'VB', 'DT', 'NN', 'NN']\n['from', 'their', 'efforts', 'on', 'Wikipedia'] ['IN', 'PRP$', 'NNS', 'IN', 'NNP']\n['emotionally', 'drained', 'from', 'their', 'efforts', 'on', 'Wikipedia', 'already'] ['RB', 'VBN', 'IN', 'PRP$', 'NNS', 'IN', 'NNP', 'RB']\n['because', '*', 'many', '*', '*', 'volunteers', 'already', 'felt', 'emotionally', 'drained', 'from', 'their', 'efforts', 'on', 'Wikipedia'] ['IN', 'NNP', 'JJ', 'NNP', 'NNP', 'NNS', 'RB', 'VBD', 'RB', 'VBN', 'IN', 'PRP$', 'NNS', 'IN', 'NNP']\n['an', 'effect'] ['DT', 'NN']\n['about', 'their', 'work'] ['IN', 'PRP$', 'NN']\n['about', 'how', 'they', 'feel', 'about', 'their', 'work'] ['IN', 'WRB', 'PRP', 'VBP', 'IN', 'PRP$', 'NN']\n['valuable', 'discoveries', 'about', 'who', 'spends', 'time', 'supporting', 'others', ',', 'how', 'they', 'think', 'about', 'how', 'they', 'feel', 'about', 'their', 'work'] ['JJ', 'NNS', 'IN', 'WP', 'VBP', 'NN', 'VBG', 'NNS', ',', 'WRB', 'PRP', 'VBP', 'IN', 'WRB', 'PRP', 'VBP', 'IN', 'PRP$', 'NN']\n['about', 'the', 'intentions', 'of', 'newcomers'] ['IN', 'DT', 'NNS', 'IN', 'NNS']\n['valuable', 'discoveries', 'about', 'who', 'spends', 'time', 'supporting', 'others'] ['JJ', 'NNS', 'IN', 'WP', 'VBP', 'NN', 'VBG', 'NNS']\n['monitors'] ['NNS']\n['themselves', '``', 'mentors'] ['PRP', '``', 'NNS']\n['insights', 'into', 'Wikipedians'] ['NNS', 'IN', 'NNS']\n['our', 'pre-prints', 'toward', 'submission', 'for', 'publication'] ['PRP$', 'NNS', 'IN', 'NN', 'IN', 'NN']\n['feedback', 'and', 'discussion', 'as', 'we', 'move', 'our', 'pre-prints', 'toward', 'submission', 'for', 'publication'] ['NN', 'CC', 'NN', 'IN', 'PRP', 'VBP', 'PRP$', 'NNS', 'IN', 'NN', 'IN', 'NN']\n['by', 'two', 'university', 'ethics', 'boards'] ['IN', 'CD', 'NN', 'NNS', 'NNS']\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-7b00ceab52a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmessage_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg2s\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkb_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkb_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkb_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arg1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkb_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkb_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arg2s'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg2s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.07/envs/allennlp/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.07/envs/allennlp/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m    106\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.07/envs/allennlp/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \"\"\"\n\u001b[0;32m-> 1272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.07/envs/allennlp/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \"\"\"\n\u001b[0;32m-> 1326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.07/envs/allennlp/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \"\"\"\n\u001b[0;32m-> 1326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.07/envs/allennlp/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.07/envs/allennlp/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \"\"\"\n\u001b[1;32m   1356\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.07/envs/allennlp/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.07/envs/allennlp/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after_tok\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "for message_id, sentence, arg1, rel, arg2s in zip(kb_df['message_id'],kb_df['sentence'],kb_df['arg1'],kb_df['rel'],kb_df['arg2s']):\n",
    "    chunk = nltk.word_tokenize(arg2s)\n",
    "    chunk = nltk.pos_tag(chunk)\n",
    "    print(list(map(lambda x: x[0], chunk)), list(map(lambda x: x[1], chunk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "nltk.tree.Tree"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "type(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['ORES']"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "current_chunk = []\n",
    "continuous_chunk = []\n",
    "\n",
    "for subtree in sentences[1]:\n",
    "    if type(subtree) == nltk.tree.Tree:\n",
    "        current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n",
    "    elif current_chunk:\n",
    "        named_entity = \" \".join(current_chunk)\n",
    "        if named_entity not in continuous_chunk:\n",
    "            continuous_chunk.append(named_entity)\n",
    "            current_chunk = []\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "continuous_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                             message_id  \\\n0  <CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com>   \n1  <CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com>   \n2  <CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com>   \n3  <CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com>   \n4  <CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com>   \n\n                                                                                                          sentence  \\\n0  It's a great suggestion to include a list of ORES based tools for people who are not familiar with ORES itself.   \n1  It's a great suggestion to include a list of ORES based tools for people who are not familiar with ORES itself.   \n2  It's a great suggestion to include a list of ORES based tools for people who are not familiar with ORES itself.   \n3                                    For our research, we would love to interview you and test our visualizations.   \n4                                    For our research, we would love to interview you and test our visualizations.   \n\n                 arg1                 rel  \\\n0              people             are not   \n1  a great suggestion          to include   \n2                  It                  's   \n3                  we  would love to test   \n4                  we          would love   \n\n                                                                 arg2s  \\\n0                                                   familiar with ORES   \n1                                a list of ORES based tools for people   \n2  a great suggestion to include a list of ORES based tools for people   \n3                                                   our visualizations   \n4                          to test our visualizations For our research   \n\n   confidence            new_arg1             new_rel  \\\n0    0.895843              people             are not   \n1    0.925061  a great suggestion          to include   \n2    0.678369                  It                  's   \n3    0.256095            Ethan Ye  would love to test   \n4    0.380779            Ethan Ye          would love   \n\n                                                             new_arg2s  \n0                                                   familiar with ORES  \n1                                a list of ORES based tools for people  \n2  a great suggestion to include a list of ORES based tools for people  \n3                                            Ethan Ye's visualizations  \n4            to test Ethan Ye's visualizations For Ethan Ye's research  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>message_id</th>\n      <th>sentence</th>\n      <th>arg1</th>\n      <th>rel</th>\n      <th>arg2s</th>\n      <th>confidence</th>\n      <th>new_arg1</th>\n      <th>new_rel</th>\n      <th>new_arg2s</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com&gt;</td>\n      <td>It's a great suggestion to include a list of ORES based tools for people who are not familiar with ORES itself.</td>\n      <td>people</td>\n      <td>are not</td>\n      <td>familiar with ORES</td>\n      <td>0.895843</td>\n      <td>people</td>\n      <td>are not</td>\n      <td>familiar with ORES</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com&gt;</td>\n      <td>It's a great suggestion to include a list of ORES based tools for people who are not familiar with ORES itself.</td>\n      <td>a great suggestion</td>\n      <td>to include</td>\n      <td>a list of ORES based tools for people</td>\n      <td>0.925061</td>\n      <td>a great suggestion</td>\n      <td>to include</td>\n      <td>a list of ORES based tools for people</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com&gt;</td>\n      <td>It's a great suggestion to include a list of ORES based tools for people who are not familiar with ORES itself.</td>\n      <td>It</td>\n      <td>'s</td>\n      <td>a great suggestion to include a list of ORES based tools for people</td>\n      <td>0.678369</td>\n      <td>It</td>\n      <td>'s</td>\n      <td>a great suggestion to include a list of ORES based tools for people</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com&gt;</td>\n      <td>For our research, we would love to interview you and test our visualizations.</td>\n      <td>we</td>\n      <td>would love to test</td>\n      <td>our visualizations</td>\n      <td>0.256095</td>\n      <td>Ethan Ye</td>\n      <td>would love to test</td>\n      <td>Ethan Ye's visualizations</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;CAE4fJj-un1Um+3aE1jTe9b8WQZuFLMaaFmCJ9zNtzTkuUja0Rw@mail.gmail.com&gt;</td>\n      <td>For our research, we would love to interview you and test our visualizations.</td>\n      <td>we</td>\n      <td>would love</td>\n      <td>to test our visualizations For our research</td>\n      <td>0.380779</td>\n      <td>Ethan Ye</td>\n      <td>would love</td>\n      <td>to test Ethan Ye's visualizations For Ethan Ye's research</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "kb_df = pd.read_csv('/Users/taroaso/myprojects/OpenIE/wiki-research-l/output/full_replaced_triple_from_text_part.csv',index_col=0)\n",
    "kb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OpenIEにかける\n",
    "\n",
    "from pyopenie import OpenIE5\n",
    "extractor = OpenIE5('http://localhost:8000')\n",
    "\n",
    "extractions_list = []\n",
    "for i,sentence in sentence_df['sentence'].items():\n",
    "    try:\n",
    "        extractions = extractor.extract(sentence)\n",
    "        extractions_list.append(extractions)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OIEの抽出結果を整形する\n",
    "\n",
    "rows = []\n",
    "for i, extractions in enumerate(extractions_list):\n",
    "    if extractions == []:\n",
    "        pass\n",
    "    else:\n",
    "        for extraction in extractions:\n",
    "            message_id = sentence_df['Message-ID'][i]\n",
    "            sentence = extraction['sentence']\n",
    "            confidence = extraction['confidence']\n",
    "            arg1 = extraction['extraction']['arg1']['text']\n",
    "            rel = extraction['extraction']['rel']['text']\n",
    "            arg2s_list = []\n",
    "            for arg2 in extraction['extraction']['arg2s']:\n",
    "                arg2s_list.append(arg2['text'])\n",
    "            arg2s = ' '.join(map(str, arg2s_list))\n",
    "            row = [message_id, sentence, arg1, rel, arg2s, confidence]\n",
    "            rows.append(row)\n",
    "\n",
    "# 整形結果をdataframeにする\n",
    "kb_df = pd.DataFrame(rows, columns = ['message_id','sentence', 'arg1', 'rel', 'arg2s', 'confidence'])\n",
    "kb_df.head()\n",
    "# dataframeをcsv出力する\n",
    "kb_df.to_csv('wiki-research-l/output/triple_from_text_part.csv')\n",
    "\n",
    "# dataframeをRDBのテーブルにする\n",
    "#from db import connect\n",
    "#engine = connect()\n",
    "#kb_df.to_sql(name='kb_wiki_research_l_text',con=engine,if_exists='replace',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# triple中の代名詞youの候補の辞書を作るために，Greetingsの行からYouの候補を取り出す．\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "you = {}\n",
    "for mail in bodies['Bodies']:\n",
    "    idx = mail['idx']\n",
    "    for greetings in mail['Greetings']:\n",
    "        doc = nlp(greetings[0])\n",
    "        you[idx] = [(X.text, X.label_) for X in doc.ents if X.label_ not in ['DATE','TIME','PERCENT','MONEY','QUANTITY','ORDINAL','CARDINAL']]\n",
    "        if you[idx] == []:\n",
    "            del you[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tripleのI, MY, ME, WE, OUR, USの代名詞をSenderに置き換えるための辞書\n",
    "refer_you = {}\n",
    "for idx, candidate in you.items():\n",
    "    message_id = mail_df[\"Message-ID\"][idx]\n",
    "    refer_you[message_id] = {\"YOU\":candidate[0][0], \"YOUR\":candidate[0][0] + '\\'s'}\n",
    "\n",
    "# tripleのI, MY, ME, WE, OUR, USの代名詞をSenderに置き換えるための辞書\n",
    "refer_pronoun = {}\n",
    "for row in kb_df.values:\n",
    "    message_id = row[0]\n",
    "    sender = mail_df[mail_df['Message-ID']==message_id]['From'].values[0]\n",
    "    start = re.search(r'(\\(.+\\))',sender).start()\n",
    "    end = re.search(r'(\\(.+\\))',sender).end()\n",
    "    sender = sender[start+1:end-1]\n",
    "    refer_pronoun[message_id]={'I':sender, 'MY':sender + '\\'s', 'ME':sender, 'WE':sender, 'OUR':sender + '\\'s', 'US':sender}\n",
    "\n",
    "# 2つの辞書を結合する\n",
    "for key, value in refer_you.items():\n",
    "    if key in refer_pronoun:\n",
    "        d = refer_pronoun[key]\n",
    "        d.update(value)\n",
    "        refer_pronoun[key] = d\n",
    "    else:\n",
    "        refer_pronoun[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Senderに置き換えるための辞書を使って実際に置き換える\n",
    "replaced_rows = []\n",
    "for row in kb_df.values:\n",
    "    message_id = row[0]\n",
    "    arg1 = row[2].split()\n",
    "    rel = row[3].split()\n",
    "    arg2s = row[4].split()\n",
    "    replaced = []\n",
    "    # arg1の置き換え\n",
    "    for i, word in enumerate(arg1):\n",
    "        sender = refer_pronoun[message_id].get(word.upper())\n",
    "        if sender is None:\n",
    "            continue\n",
    "        else:\n",
    "            arg1[i] = sender\n",
    "    new_arg1 = ' '.join(arg1)\n",
    "    replaced.append(new_arg1)\n",
    "    # relの置き換え\n",
    "    for i, word in enumerate(rel):\n",
    "        sender = refer_pronoun[message_id].get(word.upper())\n",
    "        if sender is None:\n",
    "            continue\n",
    "        else:\n",
    "            rel[i] = sender\n",
    "    new_rel = ' '.join(rel)\n",
    "    replaced.append(new_rel)\n",
    "    # arg2sの置き換え\n",
    "    for i, word in enumerate(arg2s):\n",
    "        sender = refer_pronoun[message_id].get(word.upper())\n",
    "        if sender is None:\n",
    "            continue\n",
    "        else:\n",
    "            arg2s[i] = sender\n",
    "    new_arg2s = ' '.join(arg2s)\n",
    "    replaced.append(new_arg2s)\n",
    "    # [new_arg1, new_rel, new_arg2s]を1行として追加\n",
    "    replaced_rows.append(replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_triples = pd.DataFrame(replaced_rows,columns=['new_arg1','new_rel','new_arg2s'])\n",
    "kb_df = pd.concat([kb_df, replaced_triples],axis=1)\n",
    "kb_df.to_csv('/Users/taroaso/myprojects/OpenIE/wiki-research-l/output/full_replaced_triple_from_text_part.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from entityLinking import tagme, confidentAnnotations, mediaWiki\n",
    "\n",
    "entity_list = []\n",
    "for i, sentence in sentence_df['sentence'].items():\n",
    "    json_res = tagme(sentence)\n",
    "    for candidate in json_res['annotations']:\n",
    "        if candidate['rho'] >= 0.3:\n",
    "            entity_list.append([i, candidate['spot'],candidate['rho'],candidate['id'],candidate['title']])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# 抽出したentityをdataframeに格納する\n",
    "rows = []\n",
    "for row in entity_list:\n",
    "    rows.append([sentence_df['Message-ID'][row[0]], row[1], row[2], row[3], row[4]])\n",
    "entity_df = pd.DataFrame(rows,columns=['Message-ID','spot','rho','id','title'])\n",
    "entity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entity_df = pd.read_csv('wiki-research-l/output/entity_df.csv', index_col=0)\n",
    "entity_df[entity_df['rho'] < 0.3][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extraction import entityExtraction\n",
    "\n",
    "ner_list = []\n",
    "for i, sentence in sentence_df['sentence'].items():\n",
    "    entities = entityExtraction(sentence)\n",
    "    ner_list.append(entities)\n",
    "len(ner_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}